"""
í•œêµ­ì–´ AI íŒŒì´í”„ë¼ì¸: KoBERT + KcELECTRA ì¡°í•©
- KoBERT: 1ì°¨ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ë° ê°ì • í•„í„°ë§
- KcELECTRA: 2ì°¨ ì •ë°€ ê°ì • ë¶„ì„
- ëª©í‘œ: OpenAI ë¹„ìš© ì ˆê° + ì¹´í…Œê³ ë¦¬ í¸í–¥ í•´ê²°
"""

import logging
import torch
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    BertTokenizer, BertForSequenceClassification,
    ElectraTokenizer, ElectraForSequenceClassification
)
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
from sklearn.preprocessing import LabelEncoder
import pickle
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class KoreanAIPipeline:
    """
    KoBERT + KcELECTRA ê¸°ë°˜ í•œêµ­ì–´ ë‰´ìŠ¤ ë¶„ì„ íŒŒì´í”„ë¼ì¸
    - ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (KoBERT)
    - ê°ì • ë¶„ì„ (KoBERT â†’ KcELECTRA)
    - OpenAI ëŒ€ì²´ë¡œ ë¹„ìš© ì ˆê°
    """

    def __init__(self):
        """íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ğŸ”§ Korean AI Pipeline ì´ˆê¸°í™” - Device: {self.device}")

        # ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ì •ì˜ (í”„ë¡ íŠ¸ì—”ë“œì™€ ì¼ì¹˜)
        self.categories = [
            "ì¸ê³µì§€ëŠ¥", "ë¹…ë°ì´í„°", "í´ë¼ìš°ë“œ", "ë¡œë´‡", "ë¸”ë¡ì²´ì¸",
            "ë©”íƒ€ë²„ìŠ¤", "ITê¸°ì—…", "ìŠ¤íƒ€íŠ¸ì—…", "AIì„œë¹„ìŠ¤", "ì¹¼ëŸ¼"
        ]

        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self._initialize_models()

    def _initialize_models(self):
        """KoBERT + KcELECTRA ëª¨ë¸ ë¡œë“œ"""
        try:
            # 1. KoBERT for ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (1ì°¨ í•„í„°ë§)
            logger.info("ğŸ“¥ KoBERT ëª¨ë¸ ë¡œë”©...")
            self.kobert_tokenizer = BertTokenizer.from_pretrained('skt/kobert-base-v1')
            self.kobert_model = BertForSequenceClassification.from_pretrained(
                'skt/kobert-base-v1',
                num_labels=len(self.categories)
            )
            self.kobert_model.to(self.device)
            self.kobert_model.eval()

            # 2. KcELECTRA for ì •ë°€ ê°ì • ë¶„ì„ (2ì°¨ ë¶„ì„)
            logger.info("ğŸ“¥ KcELECTRA ëª¨ë¸ ë¡œë”©...")
            self.kcelectra_tokenizer = ElectraTokenizer.from_pretrained('beomi/KcELECTRA-base')
            self.kcelectra_model = ElectraForSequenceClassification.from_pretrained(
                'beomi/KcELECTRA-base',
                num_labels=3  # positive, negative, neutral
            )
            self.kcelectra_model.to(self.device)
            self.kcelectra_model.eval()

            logger.info("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            # ë°±ì—…: ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ ì‚¬ìš©
            self.use_backup_classification = True
            logger.warning("âš ï¸ ë°±ì—… ë¶„ë¥˜ ì‹œìŠ¤í…œ ì‚¬ìš©")

    def classify_category(self, title: str, content: str = "") -> Dict[str, Any]:
        """
        KoBERT ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        Args:
            title: ë‰´ìŠ¤ ì œëª©
            content: ë‰´ìŠ¤ ë³¸ë¬¸ (ì„ íƒì )
        Returns:
            Dict: ì¹´í…Œê³ ë¦¬, ì‹ ë¢°ë„ ì ìˆ˜
        """
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì œëª© + ë³¸ë¬¸ ì•ë¶€ë¶„)
            text = title
            if content:
                text += " " + content[:200]  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ 200ìë§Œ ì‚¬ìš©

            # KoBERT í† í¬ë‚˜ì´ì§•
            inputs = self.kobert_tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=128  # KoBERT ìµœì  ê¸¸ì´
            ).to(self.device)

            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.kobert_model(**inputs)
                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                predicted_class = torch.argmax(predictions, dim=-1).item()
                confidence = predictions[0][predicted_class].item()

            category = self.categories[predicted_class]

            # ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ ë°±ì—… ì‚¬ìš©
            if confidence < 0.6:
                backup_category = self._backup_keyword_classification(title, content)
                if backup_category:
                    category = backup_category
                    confidence = 0.7  # ë°±ì—… ê¸°ë³¸ ì‹ ë¢°ë„

            return {
                "category": category,
                "confidence": confidence,
                "method": "kobert" if confidence >= 0.6 else "keyword_backup"
            }

        except Exception as e:
            logger.error(f"âŒ KoBERT ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")
            # ë°±ì—… ë¶„ë¥˜
            return {
                "category": self._backup_keyword_classification(title, content),
                "confidence": 0.5,
                "method": "keyword_backup"
            }

    def analyze_sentiment_pipeline(self, title: str, content: str) -> Dict[str, Any]:
        """
        2ë‹¨ê³„ ê°ì • ë¶„ì„: KoBERT(1ì°¨) â†’ KcELECTRA(2ì°¨)
        """
        try:
            # 1ë‹¨ê³„: KoBERTë¡œ ë¹ ë¥¸ ê°ì • ìŠ¤í¬ë¦¬ë‹
            kobert_result = self._kobert_sentiment_screen(title, content)

            # 2ë‹¨ê³„: ê°ì •ì´ ëª…í™•í•˜ì§€ ì•Šê±°ë‚˜ ê°•í•œ ê²½ìš°ë§Œ KcELECTRA ì •ë°€ ë¶„ì„
            if kobert_result["needs_detailed_analysis"]:
                logger.info("ğŸ” KcELECTRA ì •ë°€ ê°ì • ë¶„ì„ ì‹¤í–‰")
                kcelectra_result = self._kcelectra_detailed_sentiment(title, content)
                return {
                    **kcelectra_result,
                    "pipeline_method": "kobert_kcelectra",
                    "kobert_screening": kobert_result
                }
            else:
                return {
                    **kobert_result,
                    "pipeline_method": "kobert_only"
                }

        except Exception as e:
            logger.error(f"âŒ ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {str(e)}")
            return {
                "sentiment": "neutral",
                "score": 0.0,
                "confidence": 0.3,
                "pipeline_method": "fallback"
            }

    def _kobert_sentiment_screen(self, title: str, content: str) -> Dict[str, Any]:
        """KoBERT 1ì°¨ ê°ì • ìŠ¤í¬ë¦¬ë‹ (ë¹ ë¥¸ í•„í„°ë§)"""
        try:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ìŠ¤í¬ë¦¬ë‹
            text = (title + " " + content[:300]).lower()

            # ê°•í•œ ê°ì • í‚¤ì›Œë“œ íƒì§€
            positive_keywords = ["ì„±ê³µ", "ì¦ê°€", "í˜¸ì¡°", "ìƒìŠ¹", "ê°œì„ ", "ë°œì „", "í˜ì‹ ", "íšê¸°ì "]
            negative_keywords = ["ì‹¤íŒ¨", "ê°ì†Œ", "í•˜ë½", "ì•…í™”", "ë¬¸ì œ", "ìœ„ê¸°", "ì¶©ê²©", "ë…¼ë€", "ë¶„ë…¸"]
            neutral_keywords = ["ë°œí‘œ", "ê³„íš", "ì˜ˆì •", "ì§„í–‰", "ê°œìµœ", "ì°¸ì—¬", "ê´€ë ¨"]

            pos_count = sum(1 for kw in positive_keywords if kw in text)
            neg_count = sum(1 for kw in negative_keywords if kw in text)
            neu_count = sum(1 for kw in neutral_keywords if kw in text)

            # ê°•í•œ ê°ì •ì´ ê°ì§€ë˜ë©´ ì •ë°€ ë¶„ì„ í•„ìš”
            needs_detailed = (pos_count >= 2 or neg_count >= 2)

            if pos_count > neg_count:
                sentiment = "positive"
                score = min(0.7, pos_count * 0.2)
            elif neg_count > pos_count:
                sentiment = "negative"
                score = max(-0.7, -neg_count * 0.2)
            else:
                sentiment = "neutral"
                score = 0.0

            return {
                "sentiment": sentiment,
                "score": score,
                "confidence": 0.6,
                "needs_detailed_analysis": needs_detailed,
                "keyword_counts": {"positive": pos_count, "negative": neg_count, "neutral": neu_count}
            }

        except Exception as e:
            logger.error(f"âŒ KoBERT ê°ì • ìŠ¤í¬ë¦¬ë‹ ì‹¤íŒ¨: {str(e)}")
            return {
                "sentiment": "neutral",
                "score": 0.0,
                "confidence": 0.3,
                "needs_detailed_analysis": False
            }

    def _kcelectra_detailed_sentiment(self, title: str, content: str) -> Dict[str, Any]:
        """KcELECTRA ì •ë°€ ê°ì • ë¶„ì„ (SNS ê¸°ë°˜ í•™ìŠµìœ¼ë¡œ ê°ì • í‘œí˜„ì— ë¯¼ê°)"""
        try:
            # ê°ì • ë¶„ì„ì— ì¤‘ìš”í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì œëª© + ë³¸ë¬¸ ì•ë¶€ë¶„)
            text = title + " " + content[:400]

            # KcELECTRA í† í¬ë‚˜ì´ì§•
            inputs = self.kcelectra_tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=256  # KcELECTRA ìµœì  ê¸¸ì´
            ).to(self.device)

            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.kcelectra_model(**inputs)
                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                predicted_class = torch.argmax(predictions, dim=-1).item()
                confidence = predictions[0][predicted_class].item()

            # í´ë˜ìŠ¤ ë§¤í•‘ (0: negative, 1: neutral, 2: positive)
            sentiment_map = {0: "negative", 1: "neutral", 2: "positive"}
            score_map = {0: -0.8, 1: 0.0, 2: 0.8}

            sentiment = sentiment_map[predicted_class]
            score = score_map[predicted_class] * confidence

            return {
                "sentiment": sentiment,
                "score": score,
                "confidence": confidence,
                "model": "kcelectra"
            }

        except Exception as e:
            logger.error(f"âŒ KcELECTRA ê°ì • ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {
                "sentiment": "neutral",
                "score": 0.0,
                "confidence": 0.3,
                "model": "fallback"
            }

    def _backup_keyword_classification(self, title: str, content: str = "") -> str:
        """ë°±ì—… í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ (ê¸°ì¡´ ë¡œì§ ê°œì„ )"""
        text = (title + " " + content).lower()

        # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ (ë” ì •ë°€í•œ ë¶„ë¥˜)
        category_keywords = {
            "ì¸ê³µì§€ëŠ¥": {
                "high": ["ai", "ì¸ê³µì§€ëŠ¥", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "neural", "ì‹ ê²½ë§"],
                "medium": ["ìë™í™”", "ì•Œê³ ë¦¬ì¦˜", "í•™ìŠµ", "ì˜ˆì¸¡", "ë¶„ì„"]
            },
            "ë¹…ë°ì´í„°": {
                "high": ["ë¹…ë°ì´í„°", "ë°ì´í„°", "analytics", "ë¶„ì„", "ë°ì´í„°ë² ì´ìŠ¤"],
                "medium": ["í†µê³„", "ìˆ˜ì§‘", "ì²˜ë¦¬", "ì €ì¥", "ê´€ë¦¬"]
            },
            "í´ë¼ìš°ë“œ": {
                "high": ["í´ë¼ìš°ë“œ", "cloud", "aws", "azure", "gcp"],
                "medium": ["ì„œë²„", "í˜¸ìŠ¤íŒ…", "infra", "ì¸í”„ë¼", "ë„¤íŠ¸ì›Œí¬"]
            },
            "ë¡œë´‡": {
                "high": ["ë¡œë´‡", "robot", "ë“œë¡ ", "ìë™í™”", "ì œì¡°"],
                "medium": ["ê¸°ê³„", "ê³µì¥", "ì‚°ì—…", "ì œì–´", "ì„¼ì„œ"]
            },
            "ë¸”ë¡ì²´ì¸": {
                "high": ["ë¸”ë¡ì²´ì¸", "blockchain", "ì•”í˜¸í™”í", "ë¹„íŠ¸ì½”ì¸", "ì´ë”ë¦¬ì›€"],
                "medium": ["crypto", "nft", "ë””ì§€í„¸", "í† í°", "ì½”ì¸"]
            },
            "ë©”íƒ€ë²„ìŠ¤": {
                "high": ["ë©”íƒ€ë²„ìŠ¤", "metaverse", "ê°€ìƒí˜„ì‹¤", "vr", "ar", "ì¦ê°•í˜„ì‹¤"],
                "medium": ["ê°€ìƒ", "3d", "ê²Œì„", "ì²´í—˜", "immersive"]
            },
            "ITê¸°ì—…": {
                "high": ["itê¸°ì—…", "í…Œí¬", "tech", "ì†Œí”„íŠ¸ì›¨ì–´", "ê¸°ì—…"],
                "medium": ["íšŒì‚¬", "ìŠ¤íƒ€íŠ¸ì—…", "ê¸°ìˆ ", "ê°œë°œ", "ì„œë¹„ìŠ¤"]
            },
            "ìŠ¤íƒ€íŠ¸ì—…": {
                "high": ["ìŠ¤íƒ€íŠ¸ì—…", "startup", "ë²¤ì²˜", "íˆ¬ì", "í€ë”©"],
                "medium": ["ì°½ì—…", "ì‚¬ì—…", "í˜ì‹ ", "ì‹ ìƒ", "ì´ˆê¸°"]
            },
            "AIì„œë¹„ìŠ¤": {
                "high": ["aiì„œë¹„ìŠ¤", "í”Œë«í¼", "ì„œë¹„ìŠ¤", "ì†”ë£¨ì…˜", "ë„êµ¬"],
                "medium": ["ì•±", "application", "software", "ì‹œìŠ¤í…œ", "ë„ì…"]
            },
            "ì¹¼ëŸ¼": {
                "high": ["ì¹¼ëŸ¼", "opinion", "column", "ê¸°ê³ ", "ì‚¬ì„¤"],
                "medium": ["ì˜ê²¬", "ë…¼í‰", "ë¶„ì„", "ì „ë§", "ê²¬í•´"]
            }
        }

        # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        scores = {}
        for category, keywords in category_keywords.items():
            score = 0
            for word in keywords["high"]:
                score += text.count(word) * 3  # ê³ ê°€ì¤‘ì¹˜ í‚¤ì›Œë“œ
            for word in keywords["medium"]:
                score += text.count(word) * 1  # ì¤‘ê°€ì¤‘ì¹˜ í‚¤ì›Œë“œ
            scores[category] = score

        # ìµœê³  ì ìˆ˜ ì¹´í…Œê³ ë¦¬ ë°˜í™˜
        if max(scores.values()) > 0:
            return max(scores, key=scores.get)
        else:
            return "ì¸ê³µì§€ëŠ¥"  # ê¸°ë³¸ê°’

    def analyze_news_local(self, title: str, content: str) -> Dict[str, Any]:
        """
        í†µí•© ë‰´ìŠ¤ ë¶„ì„ (OpenAI ëŒ€ì²´)
        - ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (KoBERT)
        - ê°ì • ë¶„ì„ (KoBERT + KcELECTRA)
        - í‚¤ì›Œë“œ ì¶”ì¶œ (ë¡œì»¬ ì•Œê³ ë¦¬ì¦˜)
        """
        try:
            logger.info(f"ğŸ¤– ë¡œì»¬ AI ë¶„ì„ ì‹œì‘: {title[:30]}...")

            # 1. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            category_result = self.classify_category(title, content)

            # 2. ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸
            sentiment_result = self.analyze_sentiment_pipeline(title, content)

            # 3. ë¡œì»¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords_local(title, content)

            # 4. ì¤‘ìš”ë„ ê³„ì‚° (ë¡œì»¬ ì•Œê³ ë¦¬ì¦˜)
            importance = self._calculate_importance_local(title, content, sentiment_result)

            return {
                "category": category_result["category"],
                "category_confidence": category_result["confidence"],
                "sentiment": sentiment_result["sentiment"],
                "sentiment_score": sentiment_result["score"],
                "keywords": keywords,
                "importance": importance,
                "summary": self._generate_simple_summary(title, content),
                "analysis_method": "korean_ai_pipeline",
                "cost_saved": True
            }

        except Exception as e:
            logger.error(f"âŒ ë¡œì»¬ AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {
                "error": str(e),
                "analysis_method": "fallback"
            }

    def _extract_keywords_local(self, title: str, content: str) -> List[str]:
        """ë¡œì»¬ í‚¤ì›Œë“œ ì¶”ì¶œ (TF-IDF ê¸°ë°˜)"""
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            import re

            # í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            text = title + " " + content
            # í•œê¸€, ì˜ì–´, ìˆ«ìë§Œ ë‚¨ê¸°ê¸°
            text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', text)

            # ë¶ˆìš©ì–´ ì •ì˜
            stop_words = ['ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜í•œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ']

            # TF-IDF ë²¡í„°í™”
            vectorizer = TfidfVectorizer(
                max_features=10,
                stop_words=stop_words,
                ngram_range=(1, 2)
            )

            tfidf_matrix = vectorizer.fit_transform([text])
            feature_names = vectorizer.get_feature_names_out()
            tfidf_scores = tfidf_matrix.toarray()[0]

            # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
            keyword_scores = list(zip(feature_names, tfidf_scores))
            keyword_scores.sort(key=lambda x: x[1], reverse=True)

            keywords = [kw for kw, score in keyword_scores[:8] if score > 0.1]
            return keywords

        except Exception as e:
            logger.error(f"âŒ ë¡œì»¬ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            # ë°±ì—…: ì œëª©ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ
            import re
            words = re.findall(r'[ê°€-í£]{2,}', title)
            return words[:5]

    def _calculate_importance_local(self, title: str, content: str, sentiment_result: Dict) -> float:
        """ë¡œì»¬ ì¤‘ìš”ë„ ê³„ì‚°"""
        importance = 5.0  # ê¸°ë³¸ê°’

        # ê°ì • ê°•ë„ ë°˜ì˜
        sentiment_score = abs(sentiment_result.get("score", 0))
        importance += sentiment_score * 2

        # ì œëª© ê¸¸ì´ ë°˜ì˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ì œëª©ì€ ì¤‘ìš”ë„ ë‚®ìŒ)
        title_length = len(title)
        if 10 <= title_length <= 50:
            importance += 1.0

        # ë³¸ë¬¸ ê¸¸ì´ ë°˜ì˜
        content_length = len(content)
        if content_length > 500:
            importance += 1.0

        # ìµœëŒ€ê°’ ì œí•œ
        return min(10.0, importance)

    def _generate_simple_summary(self, title: str, content: str) -> str:
        """ê°„ë‹¨í•œ ë¡œì»¬ ìš”ì•½ ìƒì„±"""
        try:
            # ì²« 2ë¬¸ì¥ ì¶”ì¶œ ë˜ëŠ” ì œëª© í™•ì¥
            sentences = content.split('. ')
            if len(sentences) >= 2:
                return '. '.join(sentences[:2]) + '.'
            elif len(content) > 100:
                return content[:150] + '...'
            else:
                return title
        except:
            return title

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
_korean_ai_pipeline = None

def get_korean_ai_pipeline() -> KoreanAIPipeline:
    """Korean AI Pipeline ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _korean_ai_pipeline
    if _korean_ai_pipeline is None:
        _korean_ai_pipeline = KoreanAIPipeline()
    return _korean_ai_pipeline
