import os
import json
import hashlib
import logging
import requests
import feedparser
import uuid
from typing import List, Dict, Any, Optional
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

from app.core.config import settings
from app.db.mongodb import news_collection
from app.models.news import NewsCreate
from app.services.content_processor import get_content_processor
from app.services.langchain_service import get_langchain_service

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Make sure the data directory exists
os.makedirs(settings.DATA_DIR, exist_ok=True)


class RSSCrawler:
    """RSS Feed Crawler for collecting news articles"""

    def __init__(self, rss_feeds: List[str] = None):
        self.rss_feeds = rss_feeds or settings.RSS_FEEDS
        self.content_processor = get_content_processor()
        self.langchain_service = get_langchain_service()  # AI ìš”ì•½ ì„œë¹„ìŠ¤

    def fetch_rss_feeds(self) -> List[Dict[str, Any]]:
        """Fetch all RSS feeds and extract articles"""
        all_entries = []
        max_total_articles = 50  # ì „ì²´ ìµœëŒ€ 50ê°œë¡œ ì œí•œ

        logger.info(f"ğŸ“¡ ì‹œì‘: RSS í”¼ë“œ {len(self.rss_feeds)}ê°œ ìˆ˜ì§‘ (ìµœëŒ€ {max_total_articles}ê°œ ê¸°ì‚¬)")

        for feed_url in self.rss_feeds:
            try:
                logger.info(f"ğŸ“¥ RSS í”¼ë“œ ê°€ì ¸ì˜¤ëŠ” ì¤‘: {feed_url}")
                feed = feedparser.parse(feed_url)

                if hasattr(feed, 'status') and feed.status != 200:
                    logger.error(f"âš ï¸ RSS í”¼ë“œ ìƒíƒœ ì˜¤ë¥˜: {feed_url}, ìƒíƒœ: {feed.status}")
                    continue

                if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                    logger.warning(f"âš ï¸ RSS í”¼ë“œì— í•­ëª© ì—†ìŒ: {feed_url}")
                    continue

                # Extract source from feed URL
                source = urlparse(feed_url).netloc.replace('www.', '').replace('feeds.', '')
                logger.info(f"âœ… í”¼ë“œ ì†ŒìŠ¤: {source}, ê¸°ì‚¬ ìˆ˜: {len(feed.entries)}ê°œ")

                # Process entries (ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ 15ê°œë¡œ ì œí•œ)
                entry_count = 0
                max_per_feed = 15
                for entry in feed.entries[:max_per_feed]:
                    try:
                        # ê¸°ë³¸ ì •ë³´ë§Œ ë¹ ë¥´ê²Œ ì¶”ì¶œí•˜ì—¬ ì €ì¥ (AI ë¶„ì„ ì—†ì´)
                        article = self._process_entry_basic(entry, source)
                        if article and not article.get('existing', False):
                            # ê¸°ë³¸ ì •ë³´ë¡œ DBì— ì €ì¥ (ë¹ ë¥¸ UI í‘œì‹œìš©)
                            try:
                                # _idê°€ ë°˜ë“œì‹œ ì¡´ì¬í•˜ë„ë¡ í™•ì¸
                                if '_id' not in article or article['_id'] is None:
                                    article['_id'] = hashlib.md5(article['url'].encode('utf-8')).hexdigest()

                                # id í•„ë“œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì • (MongoDBì—ì„œ _idë¥¼ idë¡œ ì¸ì‹í•˜ì§€ ì•Šë„ë¡)
                                article['id'] = article['_id']

                                logger.info(f"ğŸ†• ì‹ ê·œ ê¸°ì‚¬ ì €ì¥: {article['title'][:30]}...")
                                # upsertë¡œ ì¤‘ë³µ ì²˜ë¦¬
                                news_collection.update_one(
                                    {"_id": article['_id']},
                                    {"$set": article},
                                    upsert=True
                                )

                                # ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡ì— ì¶”ê°€ (AI ë¶„ì„ì€ ë‚˜ì¤‘ì— ì‚¬ìš©ìê°€ í´ë¦­í•  ë•Œ ìˆ˜í–‰)
                                all_entries.append(article)
                                entry_count += 1

                                # ì „ì²´ ìµœëŒ€ ê°œìˆ˜ í™•ì¸
                                if len(all_entries) >= max_total_articles:
                                    logger.info(f"ğŸ“Š ìµœëŒ€ ê¸°ì‚¬ ìˆ˜({max_total_articles}ê°œ) ë„ë‹¬, ìˆ˜ì§‘ ì¤‘ë‹¨")
                                    return all_entries
                            except Exception as db_error:
                                logger.error(f"âŒ ê¸°ë³¸ ê¸°ì‚¬ DB ì €ì¥ ì˜¤ë¥˜: {str(db_error)}")
                        elif article and article.get('existing', False):
                            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í•­ëª©ë„ ëª©ë¡ì— ì¶”ê°€
                            all_entries.append(article)
                            entry_count += 1
                    except Exception as e:
                        logger.error(f"âŒ í•­ëª© ì²˜ë¦¬ ì˜¤ë¥˜ {entry.get('title', 'unknown')}: {e}")
                        continue

                logger.info(f"âœ… í”¼ë“œ {source}ì—ì„œ {entry_count}ê°œ ê¸°ì‚¬ ì²˜ë¦¬í•¨")
            except Exception as e:
                logger.error(f"âŒ í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜ {feed_url}: {e}")
                continue

        logger.info(f"ğŸ“Š ì´ {len(all_entries)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")

        # ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ëŠ” ê²½ìš° ë””ë²„ê¹…
        if len(all_entries) == 0:
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤! RSS í”¼ë“œ URLê³¼ íŒŒì‹± ë¡œì§ì„ í™•ì¸í•˜ì„¸ìš”.")
        else:
            # ì²« ë²ˆì§¸ ê¸°ì‚¬ ì •ë³´ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            first_article = all_entries[0]
            logger.info(f"ğŸ” ì²« ë²ˆì§¸ ê¸°ì‚¬ ì •ë³´: ì œëª©='{first_article.get('title', 'ì œëª© ì—†ìŒ')[:30]}...', URL={first_article.get('url', 'URL ì—†ìŒ')}")

        return all_entries

    def _process_entry_basic(self, entry: Dict[str, Any], source: str) -> Optional[Dict[str, Any]]:
        """ë¹ ë¥´ê²Œ ê¸°ë³¸ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ ê¸°ì‚¬ ê°ì²´ ìƒì„± (ì½œë“œ ìŠ¤íƒ€íŠ¸ ë¬¸ì œ í•´ê²°ìš©)"""
        # Extract URL
        url = entry.get('link')
        if not url:
            return None

        # Check if article already exists in database
        existing = news_collection.find_one({"url": url})
        if existing:
            logger.info(f"ğŸ“‹ ê¸°ì‚¬ê°€ ì´ë¯¸ DBì— ì¡´ì¬í•©ë‹ˆë‹¤: {url}")
            return {
                "_id": existing["_id"],
                "id": existing["_id"],  # id í•„ë“œë„ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€
                "url": url,
                "title": entry.get('title', '').strip(),
                "source": source,
                "existing": True  # ê¸°ì¡´ ê¸°ì‚¬ì„ì„ í‘œì‹œ
            }

        # Extract published date
        published_date = None
        if 'published_parsed' in entry:
            published_date = datetime(*entry.published_parsed[:6])
        elif 'updated_parsed' in entry:
            published_date = datetime(*entry.updated_parsed[:6])
        else:
            published_date = datetime.utcnow()

        # Extract title
        title = entry.get('title', '').strip()
        if not title:
            return None

        # Extract basic summary
        summary = ''
        if 'summary' in entry:
            soup = BeautifulSoup(entry.summary, 'html.parser')
            summary = soup.get_text(separator=' ', strip=True)

        # Extract basic image (ë¹ ë¥¸ ì²˜ë¦¬ìš©)
        image_url = ''
        if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
            media_thumb = entry.media_thumbnail
            if isinstance(media_thumb, list) and len(media_thumb) > 0:
                thumb = media_thumb[0]
                if isinstance(thumb, dict) and 'url' in thumb:
                    image_url = thumb['url']

        # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ì¶œ
        categories = []
        # í•­ëª©ì˜ íƒœê·¸ë‚˜ ì¹´í…Œê³ ë¦¬ í•„ë“œê°€ ìˆëŠ” ê²½ìš°
        if hasattr(entry, 'tags') and entry.tags:
            for tag in entry.tags:
                if hasattr(tag, 'term'):
                    categories.append(tag.term)

        # ì¹´í…Œê³ ë¦¬ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
        if not categories:
            # ì†ŒìŠ¤ì™€ ì œëª©ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ 
            title_lower = title.lower() if title else ""

            # í”„ë¡ íŠ¸ì—”ë“œ ì¹´í…Œê³ ë¦¬ì™€ ì¼ì¹˜í•˜ë„ë¡ ì •ì˜
            # (ì¸ê³µì§€ëŠ¥, ë¹…ë°ì´í„°, í´ë¼ìš°ë“œ, ë¡œë´‡, ë¸”ë¡ì²´ì¸, ë©”íƒ€ë²„ìŠ¤, ITê¸°ì—…, ìŠ¤íƒ€íŠ¸ì—…, AIì„œë¹„ìŠ¤, ì¹¼ëŸ¼)
            if "ai" in title_lower or "ì¸ê³µì§€ëŠ¥" in title_lower or "ë¨¸ì‹ ëŸ¬ë‹" in title_lower or "ë”¥ëŸ¬ë‹" in title_lower:
                categories = ["ì¸ê³µì§€ëŠ¥"]
            elif "ë¹…ë°ì´í„°" in title_lower or "ë°ì´í„°" in title_lower or "data" in title_lower:
                categories = ["ë¹…ë°ì´í„°"]
            elif "í´ë¼ìš°ë“œ" in title_lower or "cloud" in title_lower:
                categories = ["í´ë¼ìš°ë“œ"]
            elif "ë¡œë´‡" in title_lower or "robot" in title_lower:
                categories = ["ë¡œë´‡"]
            elif "ë¸”ë¡ì²´ì¸" in title_lower or "ì•”í˜¸í™”í" in title_lower or "blockchain" in title_lower or "crypto" in title_lower:
                categories = ["ë¸”ë¡ì²´ì¸"]
            elif "ë©”íƒ€ë²„ìŠ¤" in title_lower or "ê°€ìƒí˜„ì‹¤" in title_lower or "ì¦ê°•í˜„ì‹¤" in title_lower or "metaverse" in title_lower or "vr" in title_lower or "ar" in title_lower:
                categories = ["ë©”íƒ€ë²„ìŠ¤"]
            elif "it" in title_lower or "ê¸°ì—…" in title_lower or "íšŒì‚¬" in title_lower or "company" in title_lower or "í…Œí¬" in title_lower:
                categories = ["ITê¸°ì—…"]
            elif "ìŠ¤íƒ€íŠ¸ì—…" in title_lower or "startup" in title_lower or "ë²¤ì²˜" in title_lower:
                categories = ["ìŠ¤íƒ€íŠ¸ì—…"]
            elif "ì„œë¹„ìŠ¤" in title_lower or "í”Œë«í¼" in title_lower or "service" in title_lower or "platform" in title_lower:
                categories = ["AIì„œë¹„ìŠ¤"]
            elif "ì¹¼ëŸ¼" in title_lower or "opinion" in title_lower or "column" in title_lower or "ê¸°ê³ " in title_lower or "ì‚¬ì„¤" in title_lower:
                categories = ["ì¹¼ëŸ¼"]
            else:
                categories = ["ì¸ê³µì§€ëŠ¥"]  # ê¸°ë³¸ê°’ì€ ì¸ê³µì§€ëŠ¥ìœ¼ë¡œ ì„¤ì •

        # ID ìƒì„± (URL í•´ì‹œ ê°’ ì‚¬ìš©)
        _id = hashlib.md5(url.encode('utf-8')).hexdigest()

        # id í•„ë“œë„ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì • (MongoDBì—ì„œ _idì™€ idë¥¼ ë™ì¼í•˜ê²Œ ìœ ì§€)
        article_id = _id

        # ë‚´ìš©(content) ì¶”ì¶œ ì‹œë„
        content = ""
        # 1. content í•„ë“œ í™•ì¸
        if hasattr(entry, 'content') and entry.content:
            if isinstance(entry.content, list) and len(entry.content) > 0:
                if hasattr(entry.content[0], 'value'):
                    content_html = entry.content[0].value
                    soup = BeautifulSoup(content_html, 'html.parser')
                    content = soup.get_text(separator=' ', strip=True)

        # 2. contentê°€ ì—†ìœ¼ë©´ description í•„ë“œ í™•ì¸
        if not content and hasattr(entry, 'description'):
            soup = BeautifulSoup(entry.description, 'html.parser')
            content = soup.get_text(separator=' ', strip=True)

        # 3. ì—¬ì „íˆ ë‚´ìš©ì´ ì—†ìœ¼ë©´ summary í•„ë“œ í™•ì¸
        if not content and summary:
            content = summary

        # 4. ê·¸ë˜ë„ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ìµœì†Œí•œ ì œëª©ì„ ë‚´ìš©ìœ¼ë¡œ ì‚¬ìš©
        if not content:
            content = title + " (ë‚´ìš© ì—†ìŒ)"

        # ê¸°ë³¸ ì •ë³´ë§Œìœ¼ë¡œ ë¹ ë¥´ê²Œ ê¸°ì‚¬ ê°ì²´ ìƒì„±
        basic_article = {
            "_id": _id,
            "id": _id,  # id í•„ë“œë„ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€
            "title": title,
            "url": url,
            "source": source,
            "published_date": published_date,
            "summary": summary[:500] if summary else title[:100],  # ê°„ë‹¨í•œ ìš”ì•½ë§Œ
            "image_url": image_url or "https://via.placeholder.com/300x200?text=No+Image",
            "categories": categories,
            "content": content,  # ë‚´ìš© ì¶”ê°€
            "author": entry.get('author', source),  # ì‘ì„±ìê°€ ì—†ìœ¼ë©´ ì¶œì²˜ë¥¼ ì‘ì„±ìë¡œ ì‚¬ìš©
            "ai_enhanced": False,  # ì•„ì§ AI ì²˜ë¦¬ ì•ˆë¨
            "trust_score": 0.5,  # ê¸°ë³¸ê°’
            "sentiment_score": 0,  # ê¸°ë³¸ê°’
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow(),
            "is_basic_info": True  # ê¸°ë³¸ ì •ë³´ë§Œ ìˆëŠ” ìƒíƒœ í‘œì‹œ
        }

        logger.info(f"âœ… ê¸°ë³¸ ì •ë³´ ê¸°ì‚¬ ì¶”ì¶œ ì™„ë£Œ: {title[:30]}...")
        return basic_article

    def enhance_articles_with_full_content(self) -> int:
        """is_basic_info=Trueì¸ ê¸°ì‚¬ë“¤ì˜ ì›ë³¸ ë§í¬ì—ì„œ ì™„ì „í•œ ë³¸ë¬¸ê³¼ ì´ë¯¸ì§€ ì¶”ì¶œ"""
        logger.info("ğŸ”§ ê¸°ì‚¬ ë³¸ë¬¸ ë³´ê°• ì‹œì‘...")

        basic_articles = list(news_collection.find({"is_basic_info": True}))
        logger.info(f"ğŸ“‹ ë³´ê°• ëŒ€ìƒ: {len(basic_articles)}ê°œ ê¸°ì‚¬")

        enhanced_count = 0
        for article in basic_articles:
            try:
                url = article.get('url')
                if not url:
                    continue

                logger.info(f"ğŸ” ë³¸ë¬¸ ì¶”ì¶œ: {article.get('title', '')[:30]}...")

                # ì›ë³¸ ê¸°ì‚¬ì—ì„œ ë³¸ë¬¸ê³¼ ì´ë¯¸ì§€ ì¶”ì¶œ
                content_data = self._extract_article_from_url(url)

                if content_data['content'] and len(content_data['content']) > 50:
                    # DB ì—…ë°ì´íŠ¸
                    update_fields = {
                        'content': content_data['content'],
                        'is_basic_info': False,
                        'updated_at': datetime.utcnow()
                    }

                    if content_data['image_url']:
                        update_fields['image_url'] = content_data['image_url']

                    news_collection.update_one(
                        {'_id': article['_id']},
                        {'$set': update_fields}
                    )

                    enhanced_count += 1
                    logger.info(f"âœ… ë³´ê°• ì™„ë£Œ: {article.get('title', '')[:30]}...")
                else:
                    logger.warning(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {article.get('title', '')[:30]}...")

            except Exception as e:
                logger.error(f"âŒ ë³´ê°• ì˜¤ë¥˜: {str(e)}")
                continue

        logger.info(f"ğŸ‰ ë³´ê°• ì™„ë£Œ: {enhanced_count}/{len(basic_articles)}ê°œ ì²˜ë¦¬")
        return enhanced_count

    def _extract_article_from_url(self, url: str) -> Dict[str, Any]:
        """URLì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ê³¼ ì´ë¯¸ì§€ ì¶”ì¶œ"""
        result = {'content': '', 'image_url': '', 'error': None}

        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding or 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')

            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for unwanted in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                unwanted.extract()

            domain = urlparse(url).netloc.lower()
            content = ''
            image_url = ''

            # í•œê²¨ë ˆ
            if 'hani.co.kr' in domain:
                content_elem = soup.select_one('div.text, div.article-text')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)

                # í•œê²¨ë ˆ ê¸°ì‚¬ ì´ë¯¸ì§€ ì¶”ì¶œ ë°©ë²• ê°œì„ 
                # 1. ë¨¼ì € ì˜¤í”ˆê·¸ë˜í”„ ì´ë¯¸ì§€ í™•ì¸ (ëŒ€í‘œ ì´ë¯¸ì§€ë¡œ ê°€ì¥ ì í•©)
                og_image = soup.select_one('meta[property="og:image"]')
                if og_image and og_image.get('content'):
                    image_url = og_image.get('content')
                    logger.info(f"í•œê²¨ë ˆ ê¸°ì‚¬ì—ì„œ og:image ì°¾ìŒ: {image_url}")
                else:
                    # 2. ë‹¤ìŒìœ¼ë¡œ ê¸°ì‚¬ ë³¸ë¬¸ ì´ë¯¸ì§€ í™•ì¸
                    img_elem = soup.select_one('div.article-body img, div.text img, .image img, figure img')

                    # 3. ì˜¤ë””ì˜¤ ì¬ìƒ ë²„íŠ¼ ì´ë¯¸ì§€ëŠ” ì œì™¸
                    if img_elem and img_elem.get('src') and 'audio_play' not in img_elem.get('src'):
                        image_url = img_elem['src']
                        logger.info(f"í•œê²¨ë ˆ ê¸°ì‚¬ì—ì„œ ë³¸ë¬¸ ì´ë¯¸ì§€ ì°¾ìŒ: {image_url}")
                    else:
                        # 4. ì´ë¯¸ì§€ê°€ ì—†ê±°ë‚˜ ì˜¤ë””ì˜¤ ë²„íŠ¼ì¸ ê²½ìš° ë‹¤ë¥¸ ì´ë¯¸ì§€ íƒìƒ‰
                        all_images = soup.select('img')
                        for img in all_images:
                            src = img.get('src', '')
                            # ì˜¤ë””ì˜¤ ë²„íŠ¼ì´ë‚˜ ì‘ì€ ì•„ì´ì½˜ ì œì™¸
                            if src and 'audio_play' not in src and '.svg' not in src and (img.get('width', '0') == '0' or int(img.get('width', '0')) > 100):
                                image_url = src
                                logger.info(f"í•œê²¨ë ˆ ê¸°ì‚¬ì—ì„œ ëŒ€ì²´ ì´ë¯¸ì§€ ì°¾ìŒ: {image_url}")
                                break

            # ì¡°ì„ ì¼ë³´
            elif 'chosun.com' in domain:
                content_elem = soup.select_one('div.news_body, #news_body_id')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                img_elem = soup.select_one('.news_body img, .photo img')
                if img_elem and img_elem.get('src'):
                    image_url = img_elem['src']

            # ì—°í•©ë‰´ìŠ¤ - ì´ ë¶€ë¶„ì´ ë§ì”€í•˜ì‹  ì—°í•©ë‰´ìŠ¤ ì´ë¯¸ì§€ ì¶”ì¶œì…ë‹ˆë‹¤!
            elif 'yna.co.kr' in domain:
                content_elem = soup.select_one('div.story-news-body, .article-body, .story')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                # ì—°í•©ë‰´ìŠ¤ ì „ìš© ì´ë¯¸ì§€ ì¶”ì¶œ ë¡œì§
                img_selectors = ['.story img', '.article-photo img', '.photo img', '.image img']
                for selector in img_selectors:
                    img_elem = soup.select_one(selector)
                    if img_elem and img_elem.get('src'):
                        src = img_elem['src']
                        if 'placeholder' not in src.lower() and 'logo' not in src.lower():
                            if src.startswith('/'):
                                parsed = urlparse(url)
                                src = f"{parsed.scheme}://{parsed.netloc}{src}"
                            image_url = src
                            break

            # ì¼ë°˜ ì‚¬ì´íŠ¸
            if not content:
                selectors = [
                    'article', 'div.article-body', 'div.content', 'div.post-content',
                    'div.entry-content', 'div.story', 'main', '.main-content'
                ]
                for selector in selectors:
                    elem = soup.select_one(selector)
                    if elem:
                        text = elem.get_text(separator=' ', strip=True)
                        if len(text) > 100:
                            content = text
                            break

            # ì¼ë°˜ ì´ë¯¸ì§€ ì¶”ì¶œ
            if not image_url:
                img_selectors = ['article img', 'div.content img', '.main-image img']
                for selector in img_selectors:
                    img = soup.select_one(selector)
                    if img and img.get('src'):
                        src = img['src']
                        if 'placeholder' not in src.lower():
                            if src.startswith('/'):
                                parsed = urlparse(url)
                                src = f"{parsed.scheme}://{parsed.netloc}{src}"
                            image_url = src
                            break

            # ë³¸ë¬¸ì´ ì§§ìœ¼ë©´ p íƒœê·¸ë“¤ ì¡°í•©
            if len(content) < 100:
                paragraphs = soup.select('p')
                content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])

            result['content'] = content
            result['image_url'] = image_url

            logger.info(f"ğŸ“„ ì¶”ì¶œ ê²°ê³¼: ë³¸ë¬¸ {len(content)}ì, ì´ë¯¸ì§€ {'ìˆìŒ' if image_url else 'ì—†ìŒ'}")

        except Exception as e:
            logger.error(f"âŒ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {str(e)}")
            result['error'] = str(e)

        return result

    def enhance_all_news_sources(self) -> int:
        """ëª¨ë“  RSS ì–¸ë¡ ì‚¬ ëŒ€ì‘ ê³ ê¸‰ íŒŒì´í”„ë¼ì¸"""
        logger.info("ğŸš€ ì „ì²´ ì–¸ë¡ ì‚¬ ëŒ€ì‘ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")

        # ì²˜ë¦¬ ìƒíƒœ í™•ì¸
        total_articles = news_collection.count_documents({})
        basic_articles_count = news_collection.count_documents({"is_basic_info": True})
        completed_articles_count = news_collection.count_documents({"is_basic_info": False})

        logger.info(f"ğŸ“Š ì „ì²´ ê¸°ì‚¬: {total_articles}ê°œ")
        logger.info(f"â³ ë³´ê°• ëŒ€ê¸°: {basic_articles_count}ê°œ")
        logger.info(f"âœ… ë³´ê°• ì™„ë£Œ: {completed_articles_count}ê°œ")

        # ë³´ê°• ëŒ€ê¸° ì¤‘ì¸ ê¸°ì‚¬ë“¤ë§Œ ê°€ì ¸ì˜¤ê¸° (í•œ ë²ˆì— 20ê°œ ì²˜ë¦¬ë¡œ ì¦ê°€)
        basic_articles = list(news_collection.find({"is_basic_info": True}).limit(20))
        logger.info(f"ğŸ“‹ ì´ë²ˆ íšŒì°¨ ì²˜ë¦¬ ëŒ€ìƒ: {len(basic_articles)}ê°œ ê¸°ì‚¬")

        # ë³´ê°• ëŒ€ê¸° ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if len(basic_articles) == 0:
            logger.info("âœ… ëª¨ë“  ê¸°ì‚¬ ë³´ê°• ì™„ë£Œ!")
            return 0

        enhanced_count = 0
        for article in basic_articles:
            try:
                url = article.get('url')
                if not url:
                    continue

                categories = article.get('categories', [])
                category_text = f"[{','.join(categories[:2])}]" if categories else "[ì¹´í…Œê³ ë¦¬ì—†ìŒ]"
                logger.info(f"ğŸ” HTML íŒŒì‹± {category_text}: {article.get('title', '')[:30]}...")

                # ëª¨ë“  ì–¸ë¡ ì‚¬ ëŒ€ì‘ ì¶”ì¶œ
                content_data = self._extract_from_all_sources(url)

                # ê¸°ì¡´ ë‰´ìŠ¤ ë¬¸ì„œì—ì„œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                existing_news = news_collection.find_one({'_id': article['_id']})
                existing_categories = existing_news.get('categories', [])

                # ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ (ì¹´í…Œê³ ë¦¬ ì •ë³´ ë³´ì¡´)
                # ì´ ë¶€ë¶„ì€ ì‹¤ì œë¡œ í•„ìš”í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ - ì•„ë˜ update_fieldsë¡œ í†µí•©
                # news_collection.update_one(
                #     {'_id': article['_id']},
                #     {'$set': {
                #         'is_basic_info': False,
                #         'updated_at': datetime.utcnow(),
                #         'categories': existing_categories  # ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ ë³´ì¡´
                #     }}
                # )

                # ë‚´ìš©ì´ ì—†ì–´ë„ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸ ì§„í–‰
                update_fields = {
                    'is_basic_info': False,
                    'updated_at': datetime.utcnow(),
                    'categories': existing_categories  # í•­ìƒ ì¹´í…Œê³ ë¦¬ ì •ë³´ ë³´ì¡´
                }

                # ë‚´ìš©ì´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
                if content_data['content'] and len(content_data['content']) > 50:
                    update_fields['content'] = content_data['content']

                # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
                if content_data['image_url']:
                    update_fields['image_url'] = content_data['image_url']
                    logger.info(f"ì´ë¯¸ì§€ URL ì €ì¥: {content_data['image_url']}")

                    # AI ìš”ì•½ ê²°ê³¼ë„ DBì— ì €ì¥
                    if content_data.get('ai_enhanced'):
                        update_fields['ai_enhanced'] = True
                        if content_data.get('ai_summary'):
                            update_fields['summary'] = content_data['ai_summary']
                        if content_data.get('ai_keywords'):
                            update_fields['keywords'] = content_data['ai_keywords']
                        if content_data.get('trust_score'):
                            update_fields['trust_score'] = content_data['trust_score']
                        if content_data.get('sentiment_score') is not None:
                            update_fields['sentiment_score'] = content_data['sentiment_score']
                    else:
                        update_fields['ai_enhanced'] = False

                    # ì¹´í…Œê³ ë¦¬ ì •ë³´ëŠ” ì´ë¯¸ update_fieldsì— ì„¤ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë¶ˆí•„ìš”
                    # if 'categories' not in update_fields and existing_news and 'categories' in existing_news:
                    #     update_fields['categories'] = existing_news.get('categories', [])

                    # ê¸°ì‚¬ ë‚´ìš© ì—…ë°ì´íŠ¸ (ë³´ê°• ì™„ë£Œ í‘œì‹œ ë° ì¹´í…Œê³ ë¦¬ ë³´ì¡´)
                    news_collection.update_one(
                        {'_id': article['_id']},
                        {'$set': update_fields}
                    )

                    # DB ì €ì¥ í›„ ì„ë² ë”© ìƒì„± (ì‹¤íŒ¨í•´ë„ is_basic_infoëŠ” False ìœ ì§€)
                    # ì„ì‹œë¡œ ì„ë² ë”© ìƒì„± ê±´ë„ˆë›°ê¸° - datetime ì—ëŸ¬ í•´ê²° í›„ í™œì„±í™”
                    logger.info(f"â­ï¸ ì„ë² ë”© ìƒì„± ì„ì‹œ ê±´ë„ˆë›°ê¸°: {article['_id']}")
                    # try:
                    #     from app.services.embedding_service import get_embedding_service
                    #     embedding_service = get_embedding_service()
                    #     embedding_result = embedding_service.create_embeddings_for_news(article['_id'])
                    #     if embedding_result:
                    #         logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {article['_id']}")
                    #     else:
                    #         logger.warning(f"âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {article['_id']}")
                    # except Exception as embed_error:
                    #     logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {str(embed_error)}")

                    enhanced_count += 1
                    logger.info(f"âœ… ì „ì²´ ì–¸ë¡ ì‚¬ ë³´ê°• ì™„ë£Œ: {article.get('title', '')[:30]}...")
                else:
                    logger.warning(f"âš ï¸ ì „ì²´ ì–¸ë¡ ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {article.get('title', '')[:30]}...")

            except Exception as e:
                logger.error(f"âŒ ì „ì²´ ì–¸ë¡ ì‚¬ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {str(e)}")
                continue

        logger.info(f"ğŸ‰ ì „ì²´ ì–¸ë¡ ì‚¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {enhanced_count}/{len(basic_articles)}ê°œ ì²˜ë¦¬")
        return enhanced_count

    def _extract_from_all_sources(self, url: str) -> Dict[str, Any]:
        """ëª¨ë“  ì–¸ë¡ ì‚¬ ëŒ€ì‘ ë³¸ë¬¸ê³¼ ì´ë¯¸ì§€ ì¶”ì¶œ"""
        result = {'content': '', 'image_url': '', 'error': None}

        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding or 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')

            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for unwanted in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                unwanted.extract()

            domain = urlparse(url).netloc.lower()
            content = ''
            image_url = ''

            # êµ­ë‚´ ì–¸ë¡ ì‚¬ë“¤
            if 'hani.co.kr' in domain:  # í•œê²¨ë ˆ
                content_elem = soup.select_one('div.text, div.article-text')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                img_elem = soup.select_one('div.text img, .photo img')
                if img_elem and img_elem.get('src'):
                    image_url = img_elem['src']

            elif 'chosun.com' in domain:  # ì¡°ì„ ì¼ë³´
                # ì¡°ì„ ì¼ë³´ ì½˜í…ì¸  ì¶”ì¶œ ê°œì„ 
                content_elem = soup.select_one('div.news_body, #news_body_id, .article, .article-body, section#article_body')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                    logger.info(f"ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {len(content)}ì")
                else:
                    # ë‹¤ë¥¸ ì„ íƒì ì‹œë„
                    alternative_selectors = ['div.article-text', 'div.article_body', 'div.news-detail-body', 'section.article-body']
                    for selector in alternative_selectors:
                        elem = soup.select_one(selector)
                        if elem:
                            content = elem.get_text(separator=' ', strip=True)
                            logger.info(f"ì¡°ì„ ì¼ë³´ ëŒ€ì²´ ì„ íƒì({selector})ë¡œ ë³¸ë¬¸ ì¶”ì¶œ: {len(content)}ì")
                            break

                # ì˜¤í”ˆê·¸ë˜í”„ ì´ë¯¸ì§€ í™•ì¸ (ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ ì†ŒìŠ¤)
                og_image = soup.select_one('meta[property="og:image"]')
                if og_image and og_image.get('content'):
                    image_url = og_image.get('content')
                    logger.info(f"ì¡°ì„ ì¼ë³´ og:image íƒœê·¸ì—ì„œ ì´ë¯¸ì§€ ì°¾ìŒ: {image_url}")
                else:
                    # ì´ë¯¸ì§€ ìš”ì†Œ ì°¾ê¸°
                    img_elem = soup.select_one('.news_body img, .photo img, .article img, .article-img img')
                    if img_elem and img_elem.get('src'):
                        image_url = img_elem['src']
                        logger.info(f"ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ì—ì„œ ì´ë¯¸ì§€ ì°¾ìŒ: {image_url}")

            elif 'yna.co.kr' in domain:  # ì—°í•©ë‰´ìŠ¤
                content_elem = soup.select_one('div.story-news-body, .article-body, .story')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                img_selectors = ['.story img', '.article-photo img', '.photo img', '.image img']
                for selector in img_selectors:
                    img_elem = soup.select_one(selector)
                    if img_elem and img_elem.get('src'):
                        src = img_elem['src']
                        if 'placeholder' not in src.lower() and 'logo' not in src.lower():
                            if src.startswith('/'):
                                parsed = urlparse(url)
                                src = f"{parsed.scheme}://{parsed.netloc}{src}"
                            image_url = src
                            break

            elif 'news.kbs.co.kr' in domain:  # KBS
                content_elem = soup.select_one('div.detail-body, .news-content, .article-body')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                img_elem = soup.select_one('.detail-body img, .news-content img')
                if img_elem and img_elem.get('src'):
                    image_url = img_elem['src']

            elif 'ytn.co.kr' in domain:  # YTN
                content_elem = soup.select_one('div.article-txt, .news-content')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                img_elem = soup.select_one('.article-txt img, .news-content img')
                if img_elem and img_elem.get('src'):
                    image_url = img_elem['src']

            elif 'khan.co.kr' in domain:  # ê²½í–¥ì‹ ë¬¸
                content_elem = soup.select_one('div.art_body, .article-body')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                img_elem = soup.select_one('.art_body img, .article-body img')
                if img_elem and img_elem.get('src'):
                    image_url = img_elem['src']

            elif 'donga.com' in domain:  # ë™ì•„ì¼ë³´
                content_elem = soup.select_one('div.article_txt, .news_view')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                img_elem = soup.select_one('.article_txt img, .news_view img')
                if img_elem and img_elem.get('src'):
                    image_url = img_elem['src']

            # í•´ì™¸ ì–¸ë¡ ì‚¬ë“¤
            elif 'bbci.co.uk' in domain:  # BBC
                content_elem = soup.select_one('div[data-component="text-block"], .story-body')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                img_elem = soup.select_one('.story-body img, figure img')
                if img_elem and img_elem.get('src'):
                    image_url = img_elem['src']

            elif 'cnn.com' in domain:  # CNN
                content_elem = soup.select_one('div.zn-body__paragraph, .pg-rail-tall__body')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                img_elem = soup.select_one('.media__image img, .image img')
                if img_elem and img_elem.get('src'):
                    image_url = img_elem['src']

            elif 'nytimes.com' in domain:  # NYT
                content_elem = soup.select_one('section[name="articleBody"], .story-content')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                img_elem = soup.select_one('.story-content img, figure img')
                if img_elem and img_elem.get('src'):
                    image_url = img_elem['src']

            # IT/ê¸°ìˆ  ì–¸ë¡ ì‚¬ë“¤
            elif 'zdnet.co.kr' in domain:  # ZDNet
                content_elem = soup.select_one('div.view_content, .article-body')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                img_elem = soup.select_one('.view_content img, .article-body img')
                if img_elem and img_elem.get('src'):
                    image_url = img_elem['src']

            elif 'etnews.com' in domain:  # ì „ìì‹ ë¬¸
                content_elem = soup.select_one('div.article_body, .news_body')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                img_elem = soup.select_one('.article_body img, .news_body img')
                if img_elem and img_elem.get('src'):
                    image_url = img_elem['src']

            elif 'bloter.net' in domain:  # ë¸”ë¡œí„°
                content_elem = soup.select_one('div.entry-content, .post-content')
                if content_elem:
                    content = content_elem.get_text(separator=' ', strip=True)
                img_elem = soup.select_one('.entry-content img, .post-content img')
                if img_elem and img_elem.get('src'):
                    image_url = img_elem['src']

            # ì¼ë°˜ ì‚¬ì´íŠ¸ (ë‚˜ë¨¸ì§€ ëª¨ë“  ì‚¬ì´íŠ¸)
            if not content:
                selectors = [
                    'article', 'div.article-body', 'div.content', 'div.post-content',
                    'div.entry-content', 'div.story', 'main', '.main-content',
                    'div.news-content', 'div.text', '#content', '.content-body'
                ]
                for selector in selectors:
                    elem = soup.select_one(selector)
                    if elem:
                        text = elem.get_text(separator=' ', strip=True)
                        if len(text) > 100:
                            content = text
                            break

            # ì¼ë°˜ ì´ë¯¸ì§€ ì¶”ì¶œ (ìœ„ì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš°)
            if not image_url:
                # ë¨¼ì € ì˜¤í”ˆê·¸ë˜í”„ ì´ë¯¸ì§€ í™•ì¸ (ë§ì€ ì‚¬ì´íŠ¸ê°€ ì§€ì›)
                og_image = soup.select_one('meta[property="og:image"], meta[name="og:image"], meta[name="twitter:image"]')
                if og_image and og_image.get('content'):
                    image_url = og_image.get('content')
                    logger.info(f"og:image/twitter:image ë©”íƒ€ íƒœê·¸ì—ì„œ ì´ë¯¸ì§€ ì°¾ìŒ: {image_url}")

                # ì´ë¯¸ì§€ ì†ì„± í™•ì¸ (ì¼ë¶€ ì‚¬ì´íŠ¸ëŠ” ë‹¤ë¥¸ ì†ì„±ëª… ì‚¬ìš©)
                if not image_url:
                    meta_tags = soup.select('meta')
                    for meta in meta_tags:
                        property_val = meta.get('property', '').lower()
                        name_val = meta.get('name', '').lower()
                        if 'image' in property_val or 'image' in name_val:
                            if meta.get('content'):
                                image_url = meta.get('content')
                                logger.info(f"ë‹¤ë¥¸ ë©”íƒ€ ì´ë¯¸ì§€ íƒœê·¸ì—ì„œ ì´ë¯¸ì§€ ì°¾ìŒ: {image_url}")
                                break

                # RSS í”¼ë“œì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” media:content íƒœê·¸ í™•ì¸
                if not image_url:
                    media_content = soup.select_one('media\\:content, media:content')
                    if media_content and media_content.get('url'):
                        image_url = media_content.get('url')
                        logger.info(f"media:content íƒœê·¸ì—ì„œ ì´ë¯¸ì§€ ì°¾ìŒ: {image_url}")

                # itemprop="image" ì†ì„± ì°¾ê¸°
                if not image_url:
                    img_prop = soup.select_one('[itemprop="image"]')
                    if img_prop:
                        if img_prop.name == 'img' and img_prop.get('src'):
                            image_url = img_prop.get('src')
                        elif img_prop.get('content'):
                            image_url = img_prop.get('content')
                        logger.info(f"itemprop=image ì†ì„±ì—ì„œ ì´ë¯¸ì§€ ì°¾ìŒ: {image_url}")

                # ì—¬ì „íˆ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ì—¬ëŸ¬ ì„ íƒì ì‹œë„
                if not image_url:
                    # ì—¬ëŸ¬ ì„ íƒìë¡œ ì´ë¯¸ì§€ ì‹œë„ (ë” ë§ì€ ì„ íƒì ì¶”ê°€)
                    img_selectors = [
                        'article img', 'div.content img', '.main-image img',
                        'figure img', '.article-body img', '.article img',
                        '.image img', '.img img', 'div img', '.thumbnail img',
                        '.post-thumbnail img', '.featured-image img', '.entry-thumbnail img',
                        '.thumb img', '.entry img', '.wp-post-image', '.card img',
                        'img.img-responsive', 'img.img-fluid', '.img-container img'
                    ]
                    for selector in img_selectors:
                        images = soup.select(selector)
                        for img in images:
                            if img and img.get('src'):
                                src = img['src']
                                # ì œì™¸í•  ì´ë¯¸ì§€ íŒ¨í„´: ì˜¤ë””ì˜¤ ë²„íŠ¼, SVG, ì‘ì€ ì•„ì´ì½˜, ë¡œê³ , í”Œë ˆì´ìŠ¤í™€ë”
                                excluded_patterns = ['audio_play', '.svg', 'logo', 'placeholder', 'icon', 'button', 'blank.gif', 'spacer', 'spinner', 'loading']
                                if not any(pattern in src.lower() for pattern in excluded_patterns) and (src.endswith('.jpg') or src.endswith('.jpeg') or src.endswith('.png') or src.endswith('.gif') or src.endswith('.webp') or '/images/' in src.lower() or '/img/' in src.lower()):
                                    # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                                    if src.startswith('/'):
                                        parsed = urlparse(url)
                                        src = f"{parsed.scheme}://{parsed.netloc}{src}"

                                    # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸ (ê°€ëŠ¥í•œ ê²½ìš°)
                                    is_small_icon = False
                                    width = img.get('width', '0')
                                    height = img.get('height', '0')

                                    try:
                                        if width and int(width) < 100:
                                            is_small_icon = True
                                        if height and int(height) < 100:
                                            is_small_icon = True
                                    except:
                                        pass

                                    if not is_small_icon:
                                        image_url = src
                                        logger.info(f"ì í•©í•œ ì´ë¯¸ì§€ ì°¾ìŒ: {image_url}")
                                        break

                        if image_url:  # ì´ë¯¸ì§€ë¥¼ ì°¾ì•˜ìœ¼ë©´ ë£¨í”„ ì¢…ë£Œ
                            break

            # ë³¸ë¬¸ì´ ì§§ìœ¼ë©´ p íƒœê·¸ë“¤ ì¡°í•©
            if len(content) < 100:
                paragraphs = soup.select('p')
                content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])

            result['content'] = content
            result['image_url'] = image_url

            # ê¸°ì¡´ AI ìš”ì•½ ë¡œì§ ì¶”ê°€ (ì¶©ë¶„í•œ ê¸¸ì´ì˜ ë³¸ë¬¸ì´ ìˆì„ ë•Œë§Œ, ê·¸ë¦¬ê³  AI ìš”ì•½ì´ ì—†ì„ ë•Œë§Œ)
            # ìš”ì•½ ì¤‘ë³µ ë°©ì§€: DBì—ì„œ í•´ë‹¹ ê¸°ì‚¬ì˜ ìš”ì•½ ì •ë³´ í™•ì¸
            try:
                url_hash = hashlib.md5(url.encode()).hexdigest()
                existing_article = news_collection.find_one({"url_hash": url_hash})
                has_existing_summary = existing_article and existing_article.get("summary") and len(existing_article.get("summary", "")) > 50

                if has_existing_summary:
                    # ê¸°ì¡´ ìš”ì•½ ì •ë³´ ì¬ì‚¬ìš©
                    logger.info("ğŸ”„ ê¸°ì¡´ AI ìš”ì•½ ì¬ì‚¬ìš©")
                    result['ai_summary'] = existing_article.get("summary", "")
                    result['ai_keywords'] = existing_article.get("keywords", [])
                    result['trust_score'] = existing_article.get("trust_score", 0.5)
                    result['sentiment_score'] = existing_article.get("sentiment_score", 0)
                    result['ai_enhanced'] = existing_article.get("ai_enhanced", False)
                    logger.info(f"âœ… ê¸°ì¡´ AI ìš”ì•½ ì ìš©ë¨")
                elif content and len(content) >= 300 and not result.get('ai_summary'):
                    # ìƒˆ ìš”ì•½ ìƒì„±
                    try:
                        logger.info(f"ğŸ¤– AI ìš”ì•½ ì‹œì‘: {len(content)}ì")
                        ai_result = self.langchain_service.analyze_news_sync("", content)

                        if not "error" in ai_result:
                            result['ai_summary'] = ai_result.get("summary", "")
                            result['ai_keywords'] = ai_result.get("keywords", [])
                            result['trust_score'] = min(1.0, float(ai_result.get("importance", 5)) / 10.0)

                            sentiment_label = ai_result.get("sentiment", "neutral")
                            if sentiment_label == "positive":
                                result['sentiment_score'] = 0.7
                            elif sentiment_label == "negative":
                                result['sentiment_score'] = -0.7
                            else:
                                result['sentiment_score'] = 0

                            result['ai_enhanced'] = True
                            logger.info(f"âœ… AI ìš”ì•½ ì™„ë£Œ")
                        else:
                            logger.warning(f"âš ï¸ AI ìš”ì•½ ì‹¤íŒ¨: {ai_result.get('error')}")
                            result['ai_enhanced'] = False
                    except Exception as e:
                        logger.error(f"âŒ AI ìš”ì•½ ì˜¤ë¥˜: {str(e)}")
                        result['ai_enhanced'] = False
                else:
                    logger.info("â­ï¸ AI ìš”ì•½ ê±´ë„ˆë›°ê¸°: ì¡°ê±´ ë¯¸ì¶©ì¡±(ë³¸ë¬¸ ì§§ìŒ ë˜ëŠ” ì´ë¯¸ ìš”ì•½ ìˆìŒ)")
                    result['ai_enhanced'] = False
            except Exception as e:
                logger.error(f"âŒ AI ìš”ì•½ ê²€ì‚¬ ì˜¤ë¥˜: {str(e)}")
                result['ai_enhanced'] = False
            else:
                result['ai_enhanced'] = False

            logger.info(f"ğŸ“„ HTML íŒŒì‹± ê²°ê³¼: ë³¸ë¬¸ {len(content)}ì, ì´ë¯¸ì§€ {'ìˆìŒ' if image_url else 'ì—†ìŒ'}, AI ìš”ì•½ {'ì™„ë£Œ' if result.get('ai_enhanced') else 'ìƒëµ'}")

        except Exception as e:
            logger.error(f"âŒ ì „ì²´ ì–¸ë¡ ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {str(e)}")
            result['error'] = str(e)

        return result

    def _process_entry(self, entry: Dict[str, Any], source: str) -> Optional[Dict[str, Any]]:
        """Process a single RSS entry and extract article content with enhanced processing"""
        # Extract URL
        url = entry.get('link')
        if not url:
            return None

        # Check if article already exists in database
        existing = news_collection.find_one({"url": url})
        if existing:
            logger.info(f"ğŸ“‹ ê¸°ì‚¬ê°€ ì´ë¯¸ DBì— ì¡´ì¬í•©ë‹ˆë‹¤: {url}")
            # ê¸°ì¡´ ê¸°ì‚¬ë„ ë°˜í™˜í•˜ì—¬ ì—…ë°ì´íŠ¸ ê¸°íšŒ ì œê³µ
            return {
                "_id": existing["_id"],
                "id": existing["_id"],  # id í•„ë“œë„ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€
                "url": url,
                "title": entry.get('title', '').strip(),
                "source": source,
                "existing": True  # ê¸°ì¡´ ê¸°ì‚¬ì„ì„ í‘œì‹œ
            }

        # Extract published date
        published_date = None
        if 'published_parsed' in entry:
            published_date = datetime(*entry.published_parsed[:6])
        elif 'updated_parsed' in entry:
            published_date = datetime(*entry.updated_parsed[:6])
        else:
            published_date = datetime.utcnow()

        # Extract title
        title = entry.get('title', '').strip()
        if not title:
            return None

        # Extract author
        author = None
        if 'author' in entry:
            author = entry.author

        # Extract categories/tags
        categories = []

        # íƒœê·¸ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        if 'tags' in entry:
            categories = [tag.term for tag in entry.tags if hasattr(tag, 'term')]

        # RSS ì†ŒìŠ¤ì— ë”°ë¥¸ ìë™ ì¹´í…Œê³ ë¦¬ ì§€ì •
        source_categories = {
            'yna.co.kr': ['í•œêµ­', 'ì—°í•©ë‰´ìŠ¤'],
            'news.kbs.co.kr': ['í•œêµ­', 'KBS'],
            'ytn.co.kr': ['í•œêµ­', 'YTN'],
            'hani.co.kr': ['í•œêµ­', 'í•œê²¨ë ˆ'],
            'khan.co.kr': ['í•œêµ­', 'ê²½í–¥ì‹ ë¬¸'],
            'chosun.com': ['í•œêµ­', 'ì¡°ì„ ì¼ë³´'],
            'donga.com': ['í•œêµ­', 'ë™ì•„ì¼ë³´'],
            'bbc.co.uk': ['í•´ì™¸', 'BBC', 'ì˜êµ­'],
            'cnn.com': ['í•´ì™¸', 'CNN', 'ë¯¸êµ­'],
            'nytimes.com': ['í•´ì™¸', 'NYT', 'ë¯¸êµ­', 'New York Times'],
            'reuters.com': ['í•´ì™¸', 'Reuters', 'êµ­ì œ'],
            'npr.org': ['í•´ì™¸', 'NPR', 'ë¯¸êµ­'],
            'aljazeera.com': ['í•´ì™¸', 'Al Jazeera', 'ì¤‘ë™'],
            'theguardian.com': ['í•´ì™¸', 'The Guardian', 'ì˜êµ­'],
            # IT/ê¸°ìˆ  ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì¹´í…Œê³ ë¦¬
            'zdnet.co.kr': ['IT', 'ê¸°ìˆ ', 'ZDNet'],
            'etnews.com': ['IT', 'ê¸°ìˆ ', 'ì „ìì‹ ë¬¸', 'ì¸ê³µì§€ëŠ¥', 'í´ë¼ìš°ë“œ', 'ë¹…ë°ì´í„°'],
            'bloter.net': ['IT', 'ê¸°ìˆ ', 'ë¸”ë¡œí„°', 'ì¸ê³µì§€ëŠ¥', 'ìŠ¤íƒ€íŠ¸ì—…'],
            'venturesquare.net': ['ìŠ¤íƒ€íŠ¸ì—…', 'íˆ¬ì', 'ë²¤ì²˜'],
            'hada.io': ['IT', 'ê¸°ìˆ ', 'ìŠ¤íƒ€íŠ¸ì—…'],
            'platum.kr': ['ìŠ¤íƒ€íŠ¸ì—…', 'íˆ¬ì', 'ì„œë¹„ìŠ¤'],
            'thevc.kr': ['ìŠ¤íƒ€íŠ¸ì—…', 'íˆ¬ì', 'VC'],
            'itworld.co.kr': ['IT', 'ê¸°ìˆ ', 'ì¸ê³µì§€ëŠ¥', 'í´ë¼ìš°ë“œ', 'ë¹…ë°ì´í„°'],
            'aitimes.com': ['ì¸ê³µì§€ëŠ¥', 'AI', 'ë¨¸ì‹ ëŸ¬ë‹'],
            'itfind.or.kr': ['IT', 'ê¸°ìˆ ', 'ì‚°ì—…', 'R&D'],
            'verticalplatform.kr': ['IT', 'ê¸°ìˆ ', 'ì¸ê³µì§€ëŠ¥', 'AI-ì„œë¹„ìŠ¤'],
        }

        # URLì— í¬í•¨ëœ ë„ë©”ì¸ì— ë”°ë¼ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
        for domain, cats in source_categories.items():
            if domain in url:
                categories.extend(cats)

        # ì¤‘ë³µ ì œê±°
        categories = list(set(categories))

        # Extract summary and content from entry - HTML ë§ˆí¬ì—… ì²˜ë¦¬
        summary = ''
        content = ''

        # RSS í”¼ë“œì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ
        if 'content' in entry and entry.content:
            try:
                # content:encoded íƒœê·¸ ì²˜ë¦¬
                if isinstance(entry.content, list):
                    content_text = entry.content[0].value
                else:
                    content_text = entry.content

                # HTML íŒŒì‹±í•˜ì—¬ íƒœê·¸ ì •ë¦¬
                soup = BeautifulSoup(content_text, 'html.parser')

                # ì´ë¯¸ì§€ ì¶”ì¶œì„ ìœ„í•´ ì €ì¥
                images_from_content = soup.find_all('img')

                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for tag in soup.find_all(['script', 'style', 'iframe', 'form']):
                    tag.decompose()

                # ì½˜í…ì¸  ì¶”ì¶œ
                content = soup.get_text(separator=' ', strip=True)
            except Exception as e:
                logger.error(f"ì½˜í…ì¸  ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                if isinstance(entry.content, str):
                    content = entry.content

        # summary ì²˜ë¦¬
        if 'summary' in entry:
            try:
                soup = BeautifulSoup(entry.summary, 'html.parser')
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for tag in soup.find_all(['script', 'style', 'iframe', 'form']):
                    tag.decompose()
                summary = soup.get_text(separator=' ', strip=True)
            except Exception as e:
                logger.error(f"ìš”ì•½ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                summary = entry.summary

        # description ì²˜ë¦¬ (summaryê°€ ì—†ëŠ” ê²½ìš°)
        if not summary and 'description' in entry:
            try:
                soup = BeautifulSoup(entry.description, 'html.parser')
                for tag in soup.find_all(['script', 'style', 'iframe', 'form']):
                    tag.decompose()
                summary = soup.get_text(separator=' ', strip=True)
            except:
                summary = entry.description

        # ì´ë¯¸ì§€ ì¶”ì¶œ ê°œì„  (ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì´ë¯¸ì§€ ì°¾ê¸°)
        image_url = None
        images = []

        # 1. ì½˜í…ì¸ ì—ì„œ ì´ë¯¸ì§€ ì°¾ê¸° (ìœ„ì—ì„œ ì¶”ì¶œí•œ ì´ë¯¸ì§€)
        if 'images_from_content' in locals() and images_from_content:
            for img in images_from_content[:3]:  # ìµœëŒ€ 3ê°œë§Œ ì‚¬ìš©
                if img.get('src'):
                    img_url = img['src']
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if img_url.startswith('/'):
                        parsed_url = urlparse(url)
                        img_url = f"{parsed_url.scheme}://{parsed_url.netloc}{img_url}"

                    if not image_url:  # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ëŒ€í‘œ ì´ë¯¸ì§€ë¡œ ì‚¬ìš©
                        image_url = img_url

                    images.append({
                        'src': img_url,
                        'alt': img.get('alt', title)
                    })

        # 2. media_content í™•ì¸ (ì¼ë¶€ RSS í”¼ë“œì—ì„œ ì‚¬ìš©)
        if not image_url and 'media_content' in entry:
            media = entry.get('media_content', [])
            if isinstance(media, list) and media and isinstance(media[0], dict) and 'url' in media[0]:
                image_url = media[0]['url']
                images.append({
                    'src': image_url,
                    'alt': title
                })
            elif isinstance(media, dict) and 'url' in media:
                image_url = media['url']
                images.append({
                    'src': image_url,
                    'alt': title
                })

        # 3. media:thumbnail í™•ì¸
        if not image_url and hasattr(entry, 'media_thumbnail'):
            media_thumb = entry.media_thumbnail
            if media_thumb and isinstance(media_thumb, list) and len(media_thumb) > 0:
                thumb = media_thumb[0]
                if isinstance(thumb, dict) and 'url' in thumb:
                    image_url = thumb['url']
                    images.append({
                        'src': image_url,
                        'alt': title
                    })

        # 4. ê¸°ë³¸ ì´ë¯¸ì§€ í•„ë“œ í™•ì¸
        if not image_url and hasattr(entry, 'image') and hasattr(entry.image, 'href'):
            image_url = entry.image.href
            images.append({
                'src': image_url,
                'alt': title
            })

        # ì½˜í…ì¸ ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œë„
        if 'content' in entry and entry.content:
            for content_item in entry.content:
                if 'value' in content_item:
                    soup = BeautifulSoup(content_item.value, 'html.parser')
                    for img in soup.find_all('img'):
                        if img.get('src'):
                            src = img.get('src')
                            # ìƒëŒ€ ê²½ë¡œ í™•ì¸
                            if not bool(urlparse(src).netloc):
                                src = urljoin(url, src)
                            images.append({
                                'src': src,
                                'alt': img.get('alt', title)
                            })
                            if not image_url:
                                image_url = src
                            break

        # Fetch full article content using enhanced content processor
        content_result = self._fetch_article_content(url)
        content = content_result["content"]

        # ì½˜í…ì¸ ê°€ ë¹ˆ ê²½ìš° ê¸°ë³¸ ìš”ì•½ ë˜ëŠ” ì½˜í…ì¸  ì‚¬ìš©
        if not content:
            # Try to use full content if available
            if 'content' in entry and entry.content:
                for content_item in entry.content:
                    if 'value' in content_item:
                        soup = BeautifulSoup(content_item.value, 'html.parser')
                        content = soup.get_text()
                        break

            # Otherwise use summary
            if not content and summary:
                content = summary

        # ìƒì„±ëœ ìš”ì•½ì´ ì—†ìœ¼ë©´ entryì˜ ìš”ì•½ ì‚¬ìš©
        if not content_result["summary"] and summary:
            content_result["summary"] = summary

        # ì¶”ê°€ ì´ë¯¸ì§€ ë³‘í•©
        if content_result["images"]:
            # ì´ë¯¸ ê°€ì§€ê³  ìˆëŠ” ì´ë¯¸ì§€ URL ì§‘í•©
            existing_urls = {img["src"] for img in images}
            for img in content_result["images"]:
                if img["src"] not in existing_urls:
                    images.append(img)
                    existing_urls.add(img["src"])
                    # ëŒ€í‘œ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì‚¬ìš©
                    if not image_url:
                        image_url = img["src"]

        # Generate unique ID
        _id = hashlib.md5(url.encode('utf-8')).hexdigest()

        # AIë¥¼ ì‚¬ìš©í•œ ê¸°ì‚¬ ë¶„ì„ ë° ìš”ì•½ (ê¸¸ì´ê°€ ì¶©ë¶„í•œ ê²½ìš°ë§Œ)
        ai_enhanced = False
        ai_summary = ""
        ai_keywords = []
        trust_score = 0.5  # ê¸°ë³¸ê°’
        sentiment_score = 0  # ê¸°ë³¸ê°’

        # ì½˜í…ì¸  ê¸¸ì´ê°€ ì¶©ë¶„í•œ ê²½ìš°ë§Œ AI ì²˜ë¦¬ (ë¹„ìš© ë° ì„±ëŠ¥ ìµœì í™”)
        min_content_length = 300  # ìµœì†Œ ì½˜í…ì¸  ê¸¸ì´

        if len(content) >= min_content_length:
            try:
                logger.info(f"AI ë¶„ì„ ì‹œì‘: {url}")
                # AI ìš”ì•½ ë° ë¶„ì„ (LangChain ì„œë¹„ìŠ¤ í™œìš©)
                ai_result = self.langchain_service.analyze_news_sync(title, content)

                if not "error" in ai_result:
                    # AI ìš”ì•½ ì ìš©
                    ai_summary = ai_result.get("summary", "")
                    ai_keywords = ai_result.get("keywords", [])

                    # ì¶”ê°€ ë¶„ì„ ê²°ê³¼
                    trust_score = min(1.0, float(ai_result.get("importance", 5)) / 10.0)
                    sentiment_label = ai_result.get("sentiment", "neutral")

                    # ê°ì • ìŠ¤ì½”ì–´ ê³„ì‚°
                    if sentiment_label == "positive":
                        sentiment_score = 0.7
                    elif sentiment_label == "negative":
                        sentiment_score = -0.7

                    # AI ê°•í™” ì„±ê³µ í‘œì‹œ
                    ai_enhanced = True
                    logger.info(f"AI ë¶„ì„ ì„±ê³µ: {url}")
                else:
                    logger.warning(f"AI ë¶„ì„ ì‹¤íŒ¨: {ai_result.get('error')}")
            except Exception as e:
                logger.error(f"AI ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        else:
            logger.info(f"ì½˜í…ì¸  ê¸¸ì´ ë¶€ì¡±ìœ¼ë¡œ AI ë¶„ì„ ìƒëµ ({len(content)} < {min_content_length}): {url}")

        # ìµœì¢… ìš”ì•½ ì„ íƒ (AI ìš”ì•½ ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ ìš”ì•½ ì‚¬ìš©)
        final_summary = ai_summary if ai_enhanced and ai_summary else content_result["summary"]

        # Create article object with enhanced data
        article = {
            "_id": _id,
            "title": title,
            "content": content,
            "url": url,
            "source": source,
            "published_date": published_date,
            "author": author,
            "image_url": image_url,
            "categories": categories,
            "summary": final_summary,
            "images": images,
            "word_count": content_result["word_count"] or len(content.split()),
            "keywords": ai_keywords if ai_enhanced else [],
            "ai_enhanced": ai_enhanced,
            "trust_score": trust_score,
            "sentiment_score": sentiment_score,
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow(),
            "is_basic_info": False  # ì „ì²´ ì •ë³´ ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
        }

        # ì´ë¯¸ DBì— ê¸°ë³¸ ì •ë³´ë¡œ ì €ì¥ë˜ì–´ ìˆë‹¤ë©´ ì—…ë°ì´íŠ¸
        try:
            update_result = news_collection.update_one(
                {"_id": _id},
                {"$set": article}
            )

            if update_result.modified_count > 0:
                logger.info(f"ğŸ”„ AI ë¶„ì„ ì™„ë£Œ í›„ ê¸°ì‚¬ ì—…ë°ì´íŠ¸: {title[:30]}...")
            else:
                # ì—…ë°ì´íŠ¸ëœ ê²ƒì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ì‚½ì…
                logger.info(f"ğŸ†• AI ë¶„ì„ ì™„ë£Œ ê¸°ì‚¬ ì‹ ê·œ ì €ì¥: {title[:30]}...")
                news_collection.insert_one(article)
        except Exception as db_error:
            logger.error(f"âŒ ê¸°ì‚¬ DB ì €ì¥/ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {str(db_error)}")

        return article

    def _fetch_article_content(self, url: str) -> Dict[str, Any]:
        """Fetch and extract the main content from an article URL with enhanced processing"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            # ì¸ì½”ë”© ì¶”ì • ì²˜ë¦¬
            response.encoding = response.apparent_encoding or 'utf-8'

            # ContentProcessorë¥¼ ì‚¬ìš©í•˜ì—¬ ì½˜í…ì¸  í–¥ìƒ
            enhanced_content = self.content_processor.enhance_article(response.text, url)

            result = {
                "content": enhanced_content["content"],
                "summary": enhanced_content["summary"],
                "images": enhanced_content["images"],
                "word_count": enhanced_content["word_count"]
            }

            # ë§Œì•½ ì½˜í…ì¸ ê°€ ì •ìƒì ìœ¼ë¡œ ì¶”ì¶œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ëŒ€ì²´ ë°©ë²• ì‹œë„
            if not enhanced_content["has_content"] or len(enhanced_content["content"]) < 100:
                logger.warning(f"ContentProcessor failed to extract content from {url}, trying fallback method")

                # Parse HTML - ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„
                soup = BeautifulSoup(response.text, 'html.parser')

                # Remove script and style elements
                for script in soup(["script", "style", "header", "footer", "nav", "aside"]):
                    script.extract()

                # Find article content based on common patterns
                article_content = None

                # Try to find article by semantic tags
                article_tag = soup.find('article')
                if article_tag:
                    article_content = article_tag.get_text(separator=' ', strip=True)

                # If no article tag, try common content div patterns
                if not article_content or len(article_content) < 200:
                    content_divs = soup.select('div.content, div.article-content, div.post-content, div.entry-content, div.story')
                    if content_divs:
                        article_content = content_divs[0].get_text(separator=' ', strip=True)

                # If still no content, get all paragraphs within the body
                if not article_content or len(article_content) < 200:
                    paragraphs = soup.select('p')
                    if paragraphs:
                        article_content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])

                # If still no content, use the whole body text as a last resort
                if not article_content or len(article_content) < 200:
                    if soup.body:
                        article_content = soup.body.get_text(separator=' ', strip=True)
                    else:
                        article_content = soup.get_text(separator=' ', strip=True)

                # ì´ë¯¸ì§€ ì¶”ì¶œ (ëŒ€ì²´ ë°©ë²•)
                fallback_images = []
                for img in soup.find_all('img'):
                    if img.get('src'):
                        src = img.get('src')
                        alt = img.get('alt', '')

                        # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                        if not bool(urlparse(src).netloc):
                            src = urljoin(url, src)

                        fallback_images.append({
                            'src': src,
                            'alt': alt
                        })

                # ê²°ê³¼ ì—…ë°ì´íŠ¸
                result["content"] = article_content
                # ì´ë¯¸ì§€ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ë‹¤ë©´ fallback ì´ë¯¸ì§€ ì‚¬ìš©
                if not result["images"]:
                    result["images"] = fallback_images[:5]  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©

                # ìš”ì•½ì´ ì—†ë‹¤ë©´ ìƒì„±
                if not result["summary"] and article_content:
                    result["summary"] = self.content_processor.generate_summary(article_content)

            return result

        except Exception as e:
            logger.error(f"Error fetching article content from {url}: {e}")
            return {
                "content": "",
                "summary": "",
                "images": [],
                "word_count": 0,
                "error": str(e)
            }

    def save_articles_to_db(self, articles: List[Dict[str, Any]]) -> int:
        """Save articles to MongoDB"""
        if not articles:
            logger.warning("âŒ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0

        logger.info(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— {len(articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì‹œì‘")

        # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ê¸°ì‚¬ ìˆ˜ í™•ì¸
        existing_count = news_collection.count_documents({})
        logger.info(f"ğŸ“Š í˜„ì¬ DB ê¸°ì‚¬ ìˆ˜: {existing_count}ê°œ")

        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = {}
        for article in articles:
            article_categories = article.get("categories", ["ë¯¸ë¶„ë¥˜"])
            if not article_categories:
                article_categories = ["ë¯¸ë¶„ë¥˜"]
            for category in article_categories:
                if category in category_stats:
                    category_stats[category] += 1
                else:
                    category_stats[category] = 1

        # ì¹´í…Œê³ ë¦¬ í†µê³„ ì¶œë ¥
        for category, count in category_stats.items():
            logger.info(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬ '{category}': {count}ê°œ ê¸°ì‚¬")

        saved_count = 0
        new_count = 0
        for article in articles:
            try:
                # ê¸°ì¡´ ê¸°ì‚¬ì¸ ê²½ìš° ì²˜ë¦¬ ë°©ì‹ ë³€ê²½
                if article.get('existing', False):
                    logger.info(f"ğŸ”„ ê¸°ì¡´ ê¸°ì‚¬ ê±´ë„ˆëœ€: {article.get('title', 'ì œëª© ì—†ìŒ')[:30]}...")
                    continue

                # articleì—ì„œ _id í‚¤ ì¡´ì¬ í™•ì¸
                if "_id" not in article:
                    article["_id"] = str(uuid.uuid4())  # ê³ ìœ  ID ìƒì„±
                    logger.info(f"ğŸ†” ìƒˆ ID ìƒì„±: {article['_id']}")

                logger.info(f"ğŸ“ MongoDBì— ê¸°ì‚¬ ì €ì¥ ì‹œë„: {article.get('title', 'ì œëª© ì—†ìŒ')[:30]}...")

                # MongoDBì— ì €ì¥ ì‹œë„
                result = news_collection.update_one(
                    {"_id": article["_id"]},
                    {"$set": article},
                    upsert=True
                )
                saved_count += 1

                if result.upserted_id:
                    new_count += 1
                    logger.info(f"âœ… ìƒˆ ê¸°ì‚¬ ì €ì¥ ì„±ê³µ: {article.get('title', 'ì œëª© ì—†ìŒ')[:30]}")
                else:
                    logger.info(f"ğŸ”„ ê¸°ì¡´ ê¸°ì‚¬ ì—…ë°ì´íŠ¸: {article.get('title', 'ì œëª© ì—†ìŒ')[:30]}")
            except Exception as e:
                logger.error(f"âŒ ê¸°ì‚¬ ì €ì¥ ì˜¤ë¥˜: {e}")
                if "_id" in article and "title" in article:
                    logger.error(f"   - ê¸°ì‚¬ ID: {article['_id']}, ì œëª©: {article['title'][:30]}...")
                continue

        logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ì— ì´ {saved_count}ê°œ ê¸°ì‚¬ ì €ì¥ë¨ (ì‹ ê·œ: {new_count}ê°œ, ì—…ë°ì´íŠ¸: {saved_count-new_count}ê°œ)")
        logger.info(f"ğŸ“Š ì €ì¥ í›„ DB ê¸°ì‚¬ ìˆ˜: {news_collection.count_documents({})}ê°œ")
        return saved_count

    def crawl_and_save(self) -> int:
        """Fetch RSS feeds, crawl article content, and save to database"""
        logger.info("ğŸ“¥ RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹œì‘...")
        articles = self.fetch_rss_feeds()
        logger.info(f"ğŸ“„ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(articles)}ê°œ")

        if not articles:
            logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. DB ì €ì¥ ë‹¨ê³„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return 0

        logger.info("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ì‚¬ ì €ì¥ ì‹œì‘...")
        saved_count = self.save_articles_to_db(articles)
        logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ê¸°ì‚¬ê°€ DBì— ì €ì¥ë¨")
        return saved_count


# Helper function to run crawler
# ì¶”ì¶œì— ì‹¤íŒ¨í•œ ê¸°ì‚¬ë¥¼ ê°•ì œë¡œ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
def force_update_failed_articles():
    """HTML íŒŒì‹±ì— ì‹¤íŒ¨í•œ ê¸°ì‚¬ë“¤ì˜ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ê³  is_basic_info=Falseë¡œ ì„¤ì •"""
    logger.info("ğŸ”„ ì¶”ì¶œ ì‹¤íŒ¨ ê¸°ì‚¬ ê°•ì œ ì²˜ë¦¬ ì‹œì‘...")

    # 1. ë‚´ìš©ì´ ì—†ì§€ë§Œ HTML íŒŒì‹±ì„ ì‹œë„í•œ ê¸°ì‚¬ ì°¾ê¸° (is_basic_info=True)
    failed_articles = list(news_collection.find({"is_basic_info": True}))
    logger.info(f"ğŸ“Š ì²˜ë¦¬ ëŒ€ìƒ ê¸°ì‚¬: {len(failed_articles)}ê°œ")

    updated_count = 0
    for article in failed_articles:
        try:
            # ì¡°ì„ ì¼ë³´ OpenGraph ì´ë¯¸ì§€ URL ì§ì ‘ ê²€ìƒ‰
            if "chosun.com" in article.get("url", ""):
                try:
                    import requests
                    from bs4 import BeautifulSoup

                    # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
                    response = requests.get(article["url"], timeout=10)
                    soup = BeautifulSoup(response.text, "html.parser")

                    # OpenGraph ì´ë¯¸ì§€ ì°¾ê¸°
                    og_image = soup.select_one('meta[property="og:image"]')
                    if og_image and og_image.get('content'):
                        image_url = og_image.get('content')
                        logger.info(f"ğŸ–¼ï¸ ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ OpenGraph ì´ë¯¸ì§€ ì°¾ìŒ: {image_url}")

                        # ì´ë¯¸ì§€ URL ì—…ë°ì´íŠ¸
                        news_collection.update_one(
                            {'_id': article['_id']},
                            {'$set': {
                                'image_url': image_url,
                                'is_basic_info': False,
                                'updated_at': datetime.utcnow()
                            }}
                        )
                        updated_count += 1
                        continue
                except Exception as e:
                    logger.error(f"ì¡°ì„ ì¼ë³´ OpenGraph ì´ë¯¸ì§€ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")

            # ê¸°ë³¸ ì²˜ë¦¬: is_basic_info=Falseë¡œ ì„¤ì •í•˜ì—¬ í‘œì‹œë˜ë„ë¡ í•¨
            news_collection.update_one(
                {'_id': article['_id']},
                {'$set': {
                    'is_basic_info': False,
                    'updated_at': datetime.utcnow()
                }}
            )
            updated_count += 1

        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ê°•ì œ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {str(e)}")

    logger.info(f"âœ… ì´ {updated_count}ê°œ ê¸°ì‚¬ ê°•ì œ ì²˜ë¦¬ ì™„ë£Œ")


def run_crawler() -> int:
    """Run the RSS crawler"""
    logger.info("ğŸš€ [í¬ë¡¤ëŸ¬] RSS ìˆ˜ì§‘ ì‹œì‘")
    try:
        crawler = RSSCrawler()
        logger.info("ğŸ” í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ, í¬ë¡¤ë§ ì‹œì‘...")
        articles_count = crawler.crawl_and_save()
        logger.info(f"âœ… [í¬ë¡¤ëŸ¬] RSS ìˆ˜ì§‘ ì™„ë£Œ: {articles_count}ê°œ ê¸°ì‚¬ ì €ì¥ë¨")

        # ëª¨ë“  ì–¸ë¡ ì‚¬ ëŒ€ì‘ ê³ ê¸‰ íŒŒì´í”„ë¼ì¸
        logger.info("ğŸš€ ì „ì²´ ì–¸ë¡ ì‚¬ ëŒ€ì‘ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")

        # ë” ë§ì€ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ë²ˆ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        total_enhanced = 0
        max_iterations = 2  # ìµœëŒ€ 2ë²ˆìœ¼ë¡œ ì¶•ì†Œí•˜ì—¬ API ë¹„ìš© ì ˆê°

        # í˜„ì¬ ëŒ€ê¸° ì¤‘ì¸ ê¸°ì‚¬ ìˆ˜ í™•ì¸
        pending_articles_count = news_collection.count_documents({"is_basic_info": True})
        logger.info(f"â³ ëŒ€ê¸° ì¤‘ì¸ ì „ì²´ ê¸°ì‚¬ ìˆ˜: {pending_articles_count}ê°œ")

        # ëŒ€ê¸° ì¤‘ì¸ ê¸°ì‚¬ê°€ ë§ì§€ ì•Šìœ¼ë©´ í•œ ë²ˆë§Œ ì‹¤í–‰
        if pending_articles_count <= 20:
            max_iterations = 1
            logger.info(f"ğŸ”„ ëŒ€ê¸° ê¸°ì‚¬ê°€ ì ì–´ 1íšŒë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.")

        for i in range(max_iterations):
            enhanced_count = crawler.enhance_all_news_sources()
            total_enhanced += enhanced_count
            logger.info(f"ğŸ”„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ {i+1}/{max_iterations}: {enhanced_count}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ë¨")

            # ë” ì´ìƒ ì²˜ë¦¬í•  ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
            if enhanced_count == 0:
                break

        logger.info(f"ğŸ‰ ì „ì²´ ì–¸ë¡ ì‚¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: ì´ {total_enhanced}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ë¨")

        # ì‹¤ì œ ì €ì¥ëœ ê¸°ì‚¬ ìˆ˜ í™•ì¸
        db_articles_count = news_collection.count_documents({})
        logger.info(f"ğŸ“Š ì‹¤ì œ DB ì €ì¥ ê¸°ì‚¬ ìˆ˜: {db_articles_count}ê°œ")

        # ì¹´í…Œê³ ë¦¬ ì •ë³´ ìœ ì‹¤ ê°ì§€ ë° ë¡œê¹…
        no_category_count = news_collection.count_documents({"categories": {"$exists": False}})
        empty_category_count = news_collection.count_documents({"categories": []})
        logger.info(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ì—†ëŠ” ê¸°ì‚¬: {no_category_count}ê°œ, ë¹ˆ ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬: {empty_category_count}ê°œ")

        # ì¶”ì¶œì— ì‹¤íŒ¨í•œ ê¸°ì‚¬ë¥¼ ê°•ì œë¡œ ì²˜ë¦¬
        force_update_failed_articles()

        return articles_count
    except Exception as e:
        logger.error(f"âŒ [í¬ë¡¤ëŸ¬] ì‹¤í–‰ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")
        return 0
