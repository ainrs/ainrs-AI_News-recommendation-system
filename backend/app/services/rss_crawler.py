import os
import json
import hashlib
import logging
import requests
import feedparser
from typing import List, Dict, Any, Optional
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

from app.core.config import settings
from app.db.mongodb import news_collection
from app.models.news import NewsCreate
from app.services.content_processor import get_content_processor
from app.services.langchain_service import get_langchain_service

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Make sure the data directory exists
os.makedirs(settings.DATA_DIR, exist_ok=True)


class RSSCrawler:
    """RSS Feed Crawler for collecting news articles"""

    def __init__(self, rss_feeds: List[str] = None):
        self.rss_feeds = rss_feeds or settings.RSS_FEEDS
        self.content_processor = get_content_processor()
        self.langchain_service = get_langchain_service()  # AI ìš”ì•½ ì„œë¹„ìŠ¤

    def fetch_rss_feeds(self) -> List[Dict[str, Any]]:
        """Fetch all RSS feeds and extract articles"""
        all_entries = []

        logger.info(f"ğŸ“¡ ì‹œì‘: RSS í”¼ë“œ {len(self.rss_feeds)}ê°œ ìˆ˜ì§‘")

        for feed_url in self.rss_feeds:
            try:
                logger.info(f"ğŸ“¥ RSS í”¼ë“œ ê°€ì ¸ì˜¤ëŠ” ì¤‘: {feed_url}")
                feed = feedparser.parse(feed_url)

                if hasattr(feed, 'status') and feed.status != 200:
                    logger.error(f"âš ï¸ RSS í”¼ë“œ ìƒíƒœ ì˜¤ë¥˜: {feed_url}, ìƒíƒœ: {feed.status}")
                    continue

                if not hasattr(feed, 'entries') or len(feed.entries) == 0:
                    logger.warning(f"âš ï¸ RSS í”¼ë“œì— í•­ëª© ì—†ìŒ: {feed_url}")
                    continue

                # Extract source from feed URL
                source = urlparse(feed_url).netloc.replace('www.', '').replace('feeds.', '')
                logger.info(f"âœ… í”¼ë“œ ì†ŒìŠ¤: {source}, ê¸°ì‚¬ ìˆ˜: {len(feed.entries)}ê°œ")

                # Process entries
                entry_count = 0
                for entry in feed.entries:
                    try:
                        article = self._process_entry(entry, source)
                        if article:
                            all_entries.append(article)
                            entry_count += 1
                    except Exception as e:
                        logger.error(f"âŒ í•­ëª© ì²˜ë¦¬ ì˜¤ë¥˜ {entry.get('title', 'unknown')}: {e}")
                        continue

                logger.info(f"âœ… í”¼ë“œ {source}ì—ì„œ {entry_count}ê°œ ê¸°ì‚¬ ì²˜ë¦¬í•¨")
            except Exception as e:
                logger.error(f"âŒ í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜ {feed_url}: {e}")
                continue

        logger.info(f"ğŸ“Š ì´ {len(all_entries)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_entries

    def _process_entry(self, entry: Dict[str, Any], source: str) -> Optional[Dict[str, Any]]:
        """Process a single RSS entry and extract article content with enhanced processing"""
        # Extract URL
        url = entry.get('link')
        if not url:
            return None

        # Check if article already exists in database
        existing = news_collection.find_one({"url": url})
        if existing:
            logger.debug(f"Article already exists in database: {url}")
            return None

        # Extract published date
        published_date = None
        if 'published_parsed' in entry:
            published_date = datetime(*entry.published_parsed[:6])
        elif 'updated_parsed' in entry:
            published_date = datetime(*entry.updated_parsed[:6])
        else:
            published_date = datetime.utcnow()

        # Extract title
        title = entry.get('title', '').strip()
        if not title:
            return None

        # Extract author
        author = None
        if 'author' in entry:
            author = entry.author

        # Extract categories/tags
        categories = []

        # íƒœê·¸ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        if 'tags' in entry:
            categories = [tag.term for tag in entry.tags if hasattr(tag, 'term')]

        # RSS ì†ŒìŠ¤ì— ë”°ë¥¸ ìë™ ì¹´í…Œê³ ë¦¬ ì§€ì •
        source_categories = {
            'yna.co.kr': ['í•œêµ­', 'ì—°í•©ë‰´ìŠ¤'],
            'news.kbs.co.kr': ['í•œêµ­', 'KBS'],
            'ytn.co.kr': ['í•œêµ­', 'YTN'],
            'hani.co.kr': ['í•œêµ­', 'í•œê²¨ë ˆ'],
            'khan.co.kr': ['í•œêµ­', 'ê²½í–¥ì‹ ë¬¸'],
            'chosun.com': ['í•œêµ­', 'ì¡°ì„ ì¼ë³´'],
            'donga.com': ['í•œêµ­', 'ë™ì•„ì¼ë³´'],
            'bbc.co.uk': ['í•´ì™¸', 'BBC', 'ì˜êµ­'],
            'cnn.com': ['í•´ì™¸', 'CNN', 'ë¯¸êµ­'],
            'nytimes.com': ['í•´ì™¸', 'NYT', 'ë¯¸êµ­', 'New York Times'],
            'reuters.com': ['í•´ì™¸', 'Reuters', 'êµ­ì œ'],
            'npr.org': ['í•´ì™¸', 'NPR', 'ë¯¸êµ­'],
            'aljazeera.com': ['í•´ì™¸', 'Al Jazeera', 'ì¤‘ë™'],
            'theguardian.com': ['í•´ì™¸', 'The Guardian', 'ì˜êµ­'],
            # IT/ê¸°ìˆ  ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì¹´í…Œê³ ë¦¬
            'zdnet.co.kr': ['IT', 'ê¸°ìˆ ', 'ZDNet'],
            'etnews.com': ['IT', 'ê¸°ìˆ ', 'ì „ìì‹ ë¬¸', 'ì¸ê³µì§€ëŠ¥', 'í´ë¼ìš°ë“œ', 'ë¹…ë°ì´í„°'],
            'bloter.net': ['IT', 'ê¸°ìˆ ', 'ë¸”ë¡œí„°', 'ì¸ê³µì§€ëŠ¥', 'ìŠ¤íƒ€íŠ¸ì—…'],
            'venturesquare.net': ['ìŠ¤íƒ€íŠ¸ì—…', 'íˆ¬ì', 'ë²¤ì²˜'],
            'hada.io': ['IT', 'ê¸°ìˆ ', 'ìŠ¤íƒ€íŠ¸ì—…'],
            'platum.kr': ['ìŠ¤íƒ€íŠ¸ì—…', 'íˆ¬ì', 'ì„œë¹„ìŠ¤'],
            'thevc.kr': ['ìŠ¤íƒ€íŠ¸ì—…', 'íˆ¬ì', 'VC'],
            'itworld.co.kr': ['IT', 'ê¸°ìˆ ', 'ì¸ê³µì§€ëŠ¥', 'í´ë¼ìš°ë“œ', 'ë¹…ë°ì´í„°'],
            'aitimes.com': ['ì¸ê³µì§€ëŠ¥', 'AI', 'ë¨¸ì‹ ëŸ¬ë‹'],
            'itfind.or.kr': ['IT', 'ê¸°ìˆ ', 'ì‚°ì—…', 'R&D'],
            'verticalplatform.kr': ['IT', 'ê¸°ìˆ ', 'ì¸ê³µì§€ëŠ¥', 'AI-ì„œë¹„ìŠ¤'],
        }

        # URLì— í¬í•¨ëœ ë„ë©”ì¸ì— ë”°ë¼ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
        for domain, cats in source_categories.items():
            if domain in url:
                categories.extend(cats)

        # ì¤‘ë³µ ì œê±°
        categories = list(set(categories))

        # Extract summary and content from entry - HTML ë§ˆí¬ì—… ì²˜ë¦¬
        summary = ''
        content = ''

        # RSS í”¼ë“œì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ
        if 'content' in entry and entry.content:
            try:
                # content:encoded íƒœê·¸ ì²˜ë¦¬
                if isinstance(entry.content, list):
                    content_text = entry.content[0].value
                else:
                    content_text = entry.content

                # HTML íŒŒì‹±í•˜ì—¬ íƒœê·¸ ì •ë¦¬
                soup = BeautifulSoup(content_text, 'html.parser')

                # ì´ë¯¸ì§€ ì¶”ì¶œì„ ìœ„í•´ ì €ì¥
                images_from_content = soup.find_all('img')

                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for tag in soup.find_all(['script', 'style', 'iframe', 'form']):
                    tag.decompose()

                # ì½˜í…ì¸  ì¶”ì¶œ
                content = soup.get_text(separator=' ', strip=True)
            except Exception as e:
                logger.error(f"ì½˜í…ì¸  ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                if isinstance(entry.content, str):
                    content = entry.content

        # summary ì²˜ë¦¬
        if 'summary' in entry:
            try:
                soup = BeautifulSoup(entry.summary, 'html.parser')
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for tag in soup.find_all(['script', 'style', 'iframe', 'form']):
                    tag.decompose()
                summary = soup.get_text(separator=' ', strip=True)
            except Exception as e:
                logger.error(f"ìš”ì•½ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                summary = entry.summary

        # description ì²˜ë¦¬ (summaryê°€ ì—†ëŠ” ê²½ìš°)
        if not summary and 'description' in entry:
            try:
                soup = BeautifulSoup(entry.description, 'html.parser')
                for tag in soup.find_all(['script', 'style', 'iframe', 'form']):
                    tag.decompose()
                summary = soup.get_text(separator=' ', strip=True)
            except:
                summary = entry.description

        # ì´ë¯¸ì§€ ì¶”ì¶œ ê°œì„  (ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì´ë¯¸ì§€ ì°¾ê¸°)
        image_url = None
        images = []

        # 1. ì½˜í…ì¸ ì—ì„œ ì´ë¯¸ì§€ ì°¾ê¸° (ìœ„ì—ì„œ ì¶”ì¶œí•œ ì´ë¯¸ì§€)
        if 'images_from_content' in locals() and images_from_content:
            for img in images_from_content[:3]:  # ìµœëŒ€ 3ê°œë§Œ ì‚¬ìš©
                if img.get('src'):
                    img_url = img['src']
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if img_url.startswith('/'):
                        parsed_url = urlparse(url)
                        img_url = f"{parsed_url.scheme}://{parsed_url.netloc}{img_url}"

                    if not image_url:  # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ëŒ€í‘œ ì´ë¯¸ì§€ë¡œ ì‚¬ìš©
                        image_url = img_url

                    images.append({
                        'src': img_url,
                        'alt': img.get('alt', title)
                    })

        # 2. media_content í™•ì¸ (ì¼ë¶€ RSS í”¼ë“œì—ì„œ ì‚¬ìš©)
        if not image_url and 'media_content' in entry:
            media = entry.get('media_content', [])
            if isinstance(media, list) and media and isinstance(media[0], dict) and 'url' in media[0]:
                image_url = media[0]['url']
                images.append({
                    'src': image_url,
                    'alt': title
                })
            elif isinstance(media, dict) and 'url' in media:
                image_url = media['url']
                images.append({
                    'src': image_url,
                    'alt': title
                })

        # 3. media:thumbnail í™•ì¸
        if not image_url and hasattr(entry, 'media_thumbnail'):
            media_thumb = entry.media_thumbnail
            if media_thumb and isinstance(media_thumb, list) and len(media_thumb) > 0:
                thumb = media_thumb[0]
                if isinstance(thumb, dict) and 'url' in thumb:
                    image_url = thumb['url']
                    images.append({
                        'src': image_url,
                        'alt': title
                    })

        # 4. ê¸°ë³¸ ì´ë¯¸ì§€ í•„ë“œ í™•ì¸
        if not image_url and hasattr(entry, 'image') and hasattr(entry.image, 'href'):
            image_url = entry.image.href
            images.append({
                'src': image_url,
                'alt': title
            })

        # ì½˜í…ì¸ ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œë„
        if 'content' in entry and entry.content:
            for content_item in entry.content:
                if 'value' in content_item:
                    soup = BeautifulSoup(content_item.value, 'html.parser')
                    for img in soup.find_all('img'):
                        if img.get('src'):
                            src = img.get('src')
                            # ìƒëŒ€ ê²½ë¡œ í™•ì¸
                            if not bool(urlparse(src).netloc):
                                src = urljoin(url, src)
                            images.append({
                                'src': src,
                                'alt': img.get('alt', title)
                            })
                            if not image_url:
                                image_url = src
                            break

        # Fetch full article content using enhanced content processor
        content_result = self._fetch_article_content(url)
        content = content_result["content"]

        # ì½˜í…ì¸ ê°€ ë¹ˆ ê²½ìš° ê¸°ë³¸ ìš”ì•½ ë˜ëŠ” ì½˜í…ì¸  ì‚¬ìš©
        if not content:
            # Try to use full content if available
            if 'content' in entry and entry.content:
                for content_item in entry.content:
                    if 'value' in content_item:
                        soup = BeautifulSoup(content_item.value, 'html.parser')
                        content = soup.get_text()
                        break

            # Otherwise use summary
            if not content and summary:
                content = summary

        # ìƒì„±ëœ ìš”ì•½ì´ ì—†ìœ¼ë©´ entryì˜ ìš”ì•½ ì‚¬ìš©
        if not content_result["summary"] and summary:
            content_result["summary"] = summary

        # ì¶”ê°€ ì´ë¯¸ì§€ ë³‘í•©
        if content_result["images"]:
            # ì´ë¯¸ ê°€ì§€ê³  ìˆëŠ” ì´ë¯¸ì§€ URL ì§‘í•©
            existing_urls = {img["src"] for img in images}
            for img in content_result["images"]:
                if img["src"] not in existing_urls:
                    images.append(img)
                    existing_urls.add(img["src"])
                    # ëŒ€í‘œ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì‚¬ìš©
                    if not image_url:
                        image_url = img["src"]

        # Generate unique ID
        _id = hashlib.md5(url.encode('utf-8')).hexdigest()

        # AIë¥¼ ì‚¬ìš©í•œ ê¸°ì‚¬ ë¶„ì„ ë° ìš”ì•½ (ê¸¸ì´ê°€ ì¶©ë¶„í•œ ê²½ìš°ë§Œ)
        ai_enhanced = False
        ai_summary = ""
        ai_keywords = []
        trust_score = 0.5  # ê¸°ë³¸ê°’
        sentiment_score = 0  # ê¸°ë³¸ê°’

        # ì½˜í…ì¸  ê¸¸ì´ê°€ ì¶©ë¶„í•œ ê²½ìš°ë§Œ AI ì²˜ë¦¬ (ë¹„ìš© ë° ì„±ëŠ¥ ìµœì í™”)
        min_content_length = 300  # ìµœì†Œ ì½˜í…ì¸  ê¸¸ì´

        if len(content) >= min_content_length:
            try:
                logger.info(f"AI ë¶„ì„ ì‹œì‘: {url}")
                # AI ìš”ì•½ ë° ë¶„ì„ (LangChain ì„œë¹„ìŠ¤ í™œìš©)
                ai_result = self.langchain_service.analyze_news_sync(title, content)

                if not "error" in ai_result:
                    # AI ìš”ì•½ ì ìš©
                    ai_summary = ai_result.get("summary", "")
                    ai_keywords = ai_result.get("keywords", [])

                    # ì¶”ê°€ ë¶„ì„ ê²°ê³¼
                    trust_score = min(1.0, float(ai_result.get("importance", 5)) / 10.0)
                    sentiment_label = ai_result.get("sentiment", "neutral")

                    # ê°ì • ìŠ¤ì½”ì–´ ê³„ì‚°
                    if sentiment_label == "positive":
                        sentiment_score = 0.7
                    elif sentiment_label == "negative":
                        sentiment_score = -0.7

                    # AI ê°•í™” ì„±ê³µ í‘œì‹œ
                    ai_enhanced = True
                    logger.info(f"AI ë¶„ì„ ì„±ê³µ: {url}")
                else:
                    logger.warning(f"AI ë¶„ì„ ì‹¤íŒ¨: {ai_result.get('error')}")
            except Exception as e:
                logger.error(f"AI ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        else:
            logger.info(f"ì½˜í…ì¸  ê¸¸ì´ ë¶€ì¡±ìœ¼ë¡œ AI ë¶„ì„ ìƒëµ ({len(content)} < {min_content_length}): {url}")

        # ìµœì¢… ìš”ì•½ ì„ íƒ (AI ìš”ì•½ ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ ìš”ì•½ ì‚¬ìš©)
        final_summary = ai_summary if ai_enhanced and ai_summary else content_result["summary"]

        # Create article object with enhanced data
        article = {
            "_id": _id,
            "title": title,
            "content": content,
            "url": url,
            "source": source,
            "published_date": published_date,
            "author": author,
            "image_url": image_url,
            "categories": categories,
            "summary": final_summary,
            "images": images,
            "word_count": content_result["word_count"] or len(content.split()),
            "keywords": ai_keywords if ai_enhanced else [],
            "ai_enhanced": ai_enhanced,
            "trust_score": trust_score,
            "sentiment_score": sentiment_score,
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow()
        }

        return article

    def _fetch_article_content(self, url: str) -> Dict[str, Any]:
        """Fetch and extract the main content from an article URL with enhanced processing"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml',
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            # ì¸ì½”ë”© ì¶”ì • ì²˜ë¦¬
            response.encoding = response.apparent_encoding or 'utf-8'

            # ContentProcessorë¥¼ ì‚¬ìš©í•˜ì—¬ ì½˜í…ì¸  í–¥ìƒ
            enhanced_content = self.content_processor.enhance_article(response.text, url)

            result = {
                "content": enhanced_content["content"],
                "summary": enhanced_content["summary"],
                "images": enhanced_content["images"],
                "word_count": enhanced_content["word_count"]
            }

            # ë§Œì•½ ì½˜í…ì¸ ê°€ ì •ìƒì ìœ¼ë¡œ ì¶”ì¶œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ëŒ€ì²´ ë°©ë²• ì‹œë„
            if not enhanced_content["has_content"] or len(enhanced_content["content"]) < 100:
                logger.warning(f"ContentProcessor failed to extract content from {url}, trying fallback method")

                # Parse HTML - ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„
                soup = BeautifulSoup(response.text, 'html.parser')

                # Remove script and style elements
                for script in soup(["script", "style", "header", "footer", "nav", "aside"]):
                    script.extract()

                # Find article content based on common patterns
                article_content = None

                # Try to find article by semantic tags
                article_tag = soup.find('article')
                if article_tag:
                    article_content = article_tag.get_text(separator=' ', strip=True)

                # If no article tag, try common content div patterns
                if not article_content or len(article_content) < 200:
                    content_divs = soup.select('div.content, div.article-content, div.post-content, div.entry-content, div.story')
                    if content_divs:
                        article_content = content_divs[0].get_text(separator=' ', strip=True)

                # If still no content, get all paragraphs within the body
                if not article_content or len(article_content) < 200:
                    paragraphs = soup.select('p')
                    if paragraphs:
                        article_content = ' '.join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])

                # If still no content, use the whole body text as a last resort
                if not article_content or len(article_content) < 200:
                    if soup.body:
                        article_content = soup.body.get_text(separator=' ', strip=True)
                    else:
                        article_content = soup.get_text(separator=' ', strip=True)

                # ì´ë¯¸ì§€ ì¶”ì¶œ (ëŒ€ì²´ ë°©ë²•)
                fallback_images = []
                for img in soup.find_all('img'):
                    if img.get('src'):
                        src = img.get('src')
                        alt = img.get('alt', '')

                        # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                        if not bool(urlparse(src).netloc):
                            src = urljoin(url, src)

                        fallback_images.append({
                            'src': src,
                            'alt': alt
                        })

                # ê²°ê³¼ ì—…ë°ì´íŠ¸
                result["content"] = article_content
                # ì´ë¯¸ì§€ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ë‹¤ë©´ fallback ì´ë¯¸ì§€ ì‚¬ìš©
                if not result["images"]:
                    result["images"] = fallback_images[:5]  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©

                # ìš”ì•½ì´ ì—†ë‹¤ë©´ ìƒì„±
                if not result["summary"] and article_content:
                    result["summary"] = self.content_processor.generate_summary(article_content)

            return result

        except Exception as e:
            logger.error(f"Error fetching article content from {url}: {e}")
            return {
                "content": "",
                "summary": "",
                "images": [],
                "word_count": 0,
                "error": str(e)
            }

    def save_articles_to_db(self, articles: List[Dict[str, Any]]) -> int:
        """Save articles to MongoDB"""
        if not articles:
            logger.warning("âŒ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0

        logger.info(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— {len(articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì‹œì‘")

        # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ê¸°ì‚¬ ìˆ˜ í™•ì¸
        existing_count = news_collection.count_documents({})
        logger.info(f"ğŸ“Š í˜„ì¬ DB ê¸°ì‚¬ ìˆ˜: {existing_count}ê°œ")

        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        categories = {}
        for article in articles:
            category = article.get("category", "ë¯¸ë¶„ë¥˜")
            if category in categories:
                categories[category] += 1
            else:
                categories[category] = 1

        # ì¹´í…Œê³ ë¦¬ í†µê³„ ì¶œë ¥
        for category, count in categories.items():
            logger.info(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬ '{category}': {count}ê°œ ê¸°ì‚¬")

        saved_count = 0
        new_count = 0
        for article in articles:
            try:
                result = news_collection.update_one(
                    {"_id": article["_id"]},
                    {"$set": article},
                    upsert=True
                )
                saved_count += 1
                if result.upserted_id:
                    new_count += 1
            except Exception as e:
                logger.error(f"âŒ ê¸°ì‚¬ ì €ì¥ ì˜¤ë¥˜: {e}")
                if "_id" in article and "title" in article:
                    logger.error(f"   - ê¸°ì‚¬ ID: {article['_id']}, ì œëª©: {article['title'][:30]}...")
                continue

        logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ì— ì´ {saved_count}ê°œ ê¸°ì‚¬ ì €ì¥ë¨ (ì‹ ê·œ: {new_count}ê°œ, ì—…ë°ì´íŠ¸: {saved_count-new_count}ê°œ)")
        logger.info(f"ğŸ“Š ì €ì¥ í›„ DB ê¸°ì‚¬ ìˆ˜: {news_collection.count_documents({})}ê°œ")
        return saved_count

    def crawl_and_save(self) -> int:
        """Fetch RSS feeds, crawl article content, and save to database"""
        articles = self.fetch_rss_feeds()
        return self.save_articles_to_db(articles)


# Helper function to run crawler
def run_crawler() -> int:
    """Run the RSS crawler"""
    logger.info("ğŸš€ [í¬ë¡¤ëŸ¬] RSS ìˆ˜ì§‘ ì‹œì‘")
    try:
        crawler = RSSCrawler()
        articles_count = crawler.crawl_and_save()
        logger.info(f"âœ… [í¬ë¡¤ëŸ¬] RSS ìˆ˜ì§‘ ì™„ë£Œ: {articles_count}ê°œ ê¸°ì‚¬ ì €ì¥ë¨")
        return articles_count
    except Exception as e:
        logger.error(f"âŒ [í¬ë¡¤ëŸ¬] ì‹¤í–‰ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")
        return 0
