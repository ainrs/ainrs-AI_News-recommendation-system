"""
ì‹œìŠ¤í…œ ì„±ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸
- ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •
- ê° ì„œë¹„ìŠ¤ë³„ ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë° ì²˜ë¦¬ ì†ë„ ë¶„ì„
"""

import asyncio
import time
import logging
import json
from datetime import datetime
from typing import Dict, Any, List
import sys
import os

# ë°±ì—”ë“œ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append('/home/project/backend')

from app.services.rss_crawler import RSSCrawler
from app.services.smart_filtering_service import get_smart_filtering_service
from app.services.performance_optimizer import get_performance_optimizer
from app.services.summary_cache_service import get_summary_cache_service
from app.services.parallel_processor import get_parallel_processor
from app.services.korean_ai_pipeline import get_korean_ai_pipeline
from app.db.mongodb import news_collection

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SystemPerformanceTest:
    """ì‹œìŠ¤í…œ ì„±ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸"""

    def __init__(self):
        self.performance_optimizer = get_performance_optimizer()
        self.smart_filtering = get_smart_filtering_service()
        self.cache_service = None
        self.parallel_processor = get_parallel_processor()
        self.korean_ai = get_korean_ai_pipeline()
        self.test_results = {}

        # ìºì‹œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œë„
        try:
            self.cache_service = get_summary_cache_service()
        except Exception as e:
            logger.warning(f"ìºì‹œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    async def run_comprehensive_test(self) -> Dict[str, Any]:
        """ì „ì²´ ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ ì‹œìŠ¤í…œ ì„±ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")

        start_time = time.time()
        initial_memory = self.performance_optimizer.get_memory_usage()

        test_results = {
            "test_start_time": datetime.utcnow().isoformat(),
            "initial_memory": initial_memory,
            "tests": {}
        }

        # 1. RSS í¬ë¡¤ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ“¡ RSS í¬ë¡¤ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        rss_results = await self.test_rss_crawling()
        test_results["tests"]["rss_crawling"] = rss_results

        # 2. ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ§  ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        filtering_results = await self.test_smart_filtering()
        test_results["tests"]["smart_filtering"] = filtering_results

        # 3. ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        logger.info("âš¡ ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        parallel_results = await self.test_parallel_processing()
        test_results["tests"]["parallel_processing"] = parallel_results

        # 4. ìºì‹œ ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        if self.cache_service:
            logger.info("ğŸ’¾ ìºì‹œ ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
            cache_results = await self.test_cache_system()
            test_results["tests"]["cache_system"] = cache_results

        # 5. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
        logger.info("ğŸ§® ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„")
        memory_results = await self.test_memory_optimization()
        test_results["tests"]["memory_optimization"] = memory_results

        # 6. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •
        logger.info("ğŸ”„ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ì¸¡ì •")
        pipeline_results = await self.test_full_pipeline()
        test_results["tests"]["full_pipeline"] = pipeline_results

        # ìµœì¢… ê²°ê³¼ ì •ë¦¬
        end_time = time.time()
        final_memory = self.performance_optimizer.get_memory_usage()

        test_results.update({
            "test_end_time": datetime.utcnow().isoformat(),
            "total_duration": end_time - start_time,
            "final_memory": final_memory,
            "memory_delta": final_memory["rss_mb"] - initial_memory["rss_mb"],
            "performance_summary": self.generate_performance_summary(test_results)
        })

        # ê²°ê³¼ ì €ì¥
        await self.save_test_results(test_results)

        return test_results

    async def test_rss_crawling(self) -> Dict[str, Any]:
        """RSS í¬ë¡¤ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        try:
            start_time = time.time()
            start_memory = self.performance_optimizer.get_memory_usage()

            # RSS í¬ë¡¤ëŸ¬ ìƒì„± ë° ì‹¤í–‰
            crawler = RSSCrawler()

            # ê¸°ë³¸ RSS ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
            rss_start = time.time()
            articles = crawler.fetch_rss_feeds()
            rss_duration = time.time() - rss_start

            end_memory = self.performance_optimizer.get_memory_usage()

            return {
                "success": True,
                "articles_collected": len(articles),
                "duration_seconds": rss_duration,
                "articles_per_second": len(articles) / rss_duration if rss_duration > 0 else 0,
                "memory_usage_mb": end_memory["rss_mb"] - start_memory["rss_mb"],
                "memory_efficiency": len(articles) / (end_memory["rss_mb"] - start_memory["rss_mb"] + 0.1)
            }

        except Exception as e:
            logger.error(f"RSS í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    async def test_smart_filtering(self) -> Dict[str, Any]:
        """ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        try:
            # í…ŒìŠ¤íŠ¸ìš© ê¸°ì‚¬ ë°ì´í„° ìƒì„±
            test_articles = []
            for i in range(100):
                test_articles.append({
                    "title": f"í…ŒìŠ¤íŠ¸ ê¸°ì‚¬ {i}",
                    "url": f"https://test.com/news{i}",
                    "source": "í…ŒìŠ¤íŠ¸ ì†ŒìŠ¤",
                    "published_date": datetime.utcnow(),
                    "categories": ["í…ŒìŠ¤íŠ¸"]
                })

            start_time = time.time()
            start_memory = self.performance_optimizer.get_memory_usage()

            # ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ ì‹¤í–‰
            filtered_articles = self.smart_filtering.filter_articles_smart(test_articles)

            end_time = time.time()
            end_memory = self.performance_optimizer.get_memory_usage()

            return {
                "success": True,
                "input_articles": len(test_articles),
                "filtered_articles": len(filtered_articles),
                "filtering_ratio": len(filtered_articles) / len(test_articles),
                "duration_seconds": end_time - start_time,
                "articles_per_second": len(test_articles) / (end_time - start_time),
                "memory_usage_mb": end_memory["rss_mb"] - start_memory["rss_mb"]
            }

        except Exception as e:
            logger.error(f"ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    async def test_parallel_processing(self) -> Dict[str, Any]:
        """ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        try:
            # í…ŒìŠ¤íŠ¸ìš© URL ë¦¬ìŠ¤íŠ¸
            test_urls = [
                "https://httpbin.org/delay/1",
                "https://httpbin.org/delay/1",
                "https://httpbin.org/delay/1",
                "https://httpbin.org/delay/1",
                "https://httpbin.org/delay/1"
            ]

            # ìˆœì°¨ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            sequential_start = time.time()
            sequential_results = []
            for url in test_urls:
                try:
                    # ì‹œë®¬ë ˆì´ì…˜ëœ ìˆœì°¨ ì²˜ë¦¬
                    await asyncio.sleep(0.1)  # ì‹¤ì œ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ëŒ€ì‹  ì‹œë®¬ë ˆì´ì…˜
                    sequential_results.append({"url": url, "status": "success"})
                except:
                    sequential_results.append({"url": url, "status": "failed"})
            sequential_duration = time.time() - sequential_start

            # ë³‘ë ¬ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            parallel_start = time.time()

            async def test_fetch(url):
                await asyncio.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜
                return {"url": url, "status": "success"}

            parallel_results = await asyncio.gather(
                *[test_fetch(url) for url in test_urls],
                return_exceptions=True
            )
            parallel_duration = time.time() - parallel_start

            speedup = sequential_duration / parallel_duration if parallel_duration > 0 else 0

            return {
                "success": True,
                "test_urls_count": len(test_urls),
                "sequential_duration": sequential_duration,
                "parallel_duration": parallel_duration,
                "speedup_factor": speedup,
                "parallel_efficiency": (speedup / len(test_urls)) * 100
            }

        except Exception as e:
            logger.error(f"ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    async def test_cache_system(self) -> Dict[str, Any]:
        """ìºì‹œ ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        try:
            if not self.cache_service:
                return {"success": False, "error": "ìºì‹œ ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"}

            test_content = "í…ŒìŠ¤íŠ¸ ì½˜í…ì¸  " * 100  # í…ŒìŠ¤íŠ¸ìš© ê¸´ ì½˜í…ì¸ 
            test_key = "performance_test_article"

            # ìºì‹œ ì €ì¥ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            save_start = time.time()
            await self.cache_service.save_summary(test_key, {
                "summary": "í…ŒìŠ¤íŠ¸ ìš”ì•½",
                "content": test_content,
                "timestamp": datetime.utcnow()
            })
            save_duration = time.time() - save_start

            # ìºì‹œ ì¡°íšŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            lookup_start = time.time()
            cached_data = await self.cache_service.get_summary(test_key)
            lookup_duration = time.time() - lookup_start

            # ìºì‹œ í†µê³„
            cache_stats = await self.cache_service.get_cache_stats()

            return {
                "success": True,
                "save_duration_ms": save_duration * 1000,
                "lookup_duration_ms": lookup_duration * 1000,
                "cache_hit": cached_data is not None,
                "cache_stats": cache_stats
            }

        except Exception as e:
            logger.error(f"ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    async def test_memory_optimization(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€ ì‹œë®¬ë ˆì´ì…˜
            test_data = []
            for i in range(1000):
                test_data.append({"data": "x" * 1000, "index": i})

            memory_before = self.performance_optimizer.get_memory_usage()

            # ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰
            optimization_start = time.time()
            await self.performance_optimizer.optimize_memory()
            optimization_duration = time.time() - optimization_start

            memory_after = self.performance_optimizer.get_memory_usage()

            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
            del test_data

            return {
                "success": True,
                "memory_before_mb": memory_before["rss_mb"],
                "memory_after_mb": memory_after["rss_mb"],
                "memory_freed_mb": memory_before["rss_mb"] - memory_after["rss_mb"],
                "optimization_duration_ms": optimization_duration * 1000,
                "memory_efficiency": (memory_before["rss_mb"] - memory_after["rss_mb"]) / optimization_duration
            }

        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    async def test_full_pipeline(self) -> Dict[str, Any]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        try:
            start_time = time.time()
            start_memory = self.performance_optimizer.get_memory_usage()

            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜
            pipeline_steps = {
                "rss_collection": 0,
                "smart_filtering": 0,
                "content_enhancement": 0,
                "ai_analysis": 0,
                "database_storage": 0
            }

            # ê° ë‹¨ê³„ë³„ ì‹œê°„ ì¸¡ì •
            step_start = time.time()
            await asyncio.sleep(0.1)  # RSS ìˆ˜ì§‘ ì‹œë®¬ë ˆì´ì…˜
            pipeline_steps["rss_collection"] = time.time() - step_start

            step_start = time.time()
            await asyncio.sleep(0.05)  # ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ ì‹œë®¬ë ˆì´ì…˜
            pipeline_steps["smart_filtering"] = time.time() - step_start

            step_start = time.time()
            await asyncio.sleep(0.2)  # ì½˜í…ì¸  ë³´ê°• ì‹œë®¬ë ˆì´ì…˜
            pipeline_steps["content_enhancement"] = time.time() - step_start

            step_start = time.time()
            await asyncio.sleep(0.15)  # AI ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜
            pipeline_steps["ai_analysis"] = time.time() - step_start

            step_start = time.time()
            await asyncio.sleep(0.03)  # DB ì €ì¥ ì‹œë®¬ë ˆì´ì…˜
            pipeline_steps["database_storage"] = time.time() - step_start

            total_duration = time.time() - start_time
            end_memory = self.performance_optimizer.get_memory_usage()

            return {
                "success": True,
                "total_duration_seconds": total_duration,
                "pipeline_steps": pipeline_steps,
                "memory_usage_mb": end_memory["rss_mb"] - start_memory["rss_mb"],
                "throughput_estimate": 50 / total_duration  # 50ê°œ ê¸°ì‚¬ ê¸°ì¤€
            }

        except Exception as e:
            logger.error(f"ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}

    def generate_performance_summary(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        summary = {
            "overall_status": "success",
            "key_metrics": {},
            "recommendations": []
        }

        tests = test_results.get("tests", {})

        # RSS í¬ë¡¤ë§ ì„±ëŠ¥
        if "rss_crawling" in tests and tests["rss_crawling"]["success"]:
            rss = tests["rss_crawling"]
            summary["key_metrics"]["rss_articles_per_second"] = rss.get("articles_per_second", 0)
            if rss.get("articles_per_second", 0) < 10:
                summary["recommendations"].append("RSS í¬ë¡¤ë§ ì†ë„ ê°œì„  í•„ìš”")

        # ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ íš¨ìœ¨ì„±
        if "smart_filtering" in tests and tests["smart_filtering"]["success"]:
            filtering = tests["smart_filtering"]
            summary["key_metrics"]["filtering_ratio"] = filtering.get("filtering_ratio", 0)
            if filtering.get("filtering_ratio", 0) > 0.8:
                summary["recommendations"].append("í•„í„°ë§ ê¸°ì¤€ ê°•í™” ê²€í† ")

        # ë³‘ë ¬ ì²˜ë¦¬ ê°€ì†í™”
        if "parallel_processing" in tests and tests["parallel_processing"]["success"]:
            parallel = tests["parallel_processing"]
            summary["key_metrics"]["speedup_factor"] = parallel.get("speedup_factor", 0)
            if parallel.get("speedup_factor", 0) < 3:
                summary["recommendations"].append("ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” í•„ìš”")

        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
        if "memory_optimization" in tests and tests["memory_optimization"]["success"]:
            memory = tests["memory_optimization"]
            summary["key_metrics"]["memory_freed_mb"] = memory.get("memory_freed_mb", 0)
            if memory.get("memory_freed_mb", 0) < 10:
                summary["recommendations"].append("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ê²€í† ")

        # ì „ì²´ ì²˜ë¦¬ëŸ‰
        if "full_pipeline" in tests and tests["full_pipeline"]["success"]:
            pipeline = tests["full_pipeline"]
            summary["key_metrics"]["throughput_articles_per_second"] = pipeline.get("throughput_estimate", 0)

        # ì „ì²´ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
        memory_delta = test_results.get("memory_delta", 0)
        if memory_delta > 100:  # 100MB ì´ìƒ ì¦ê°€
            summary["recommendations"].append("ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°€ëŠ¥ì„± ì ê²€ í•„ìš”")

        return summary

    async def save_test_results(self, results: Dict[str, Any]):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        try:
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            filename = f"performance_test_results_{timestamp}.json"

            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)

            logger.info(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥ë¨: {filename}")

        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì‹œìŠ¤í…œ ì„±ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")

    test_runner = SystemPerformanceTest()
    results = await test_runner.run_comprehensive_test()

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*60)

    print(f"â±ï¸  ì´ í…ŒìŠ¤íŠ¸ ì‹œê°„: {results['total_duration']:.2f}ì´ˆ")
    print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë³€í™”: {results['memory_delta']:.2f}MB")

    summary = results.get('performance_summary', {})
    key_metrics = summary.get('key_metrics', {})

    print("\nğŸ” ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ:")
    for metric, value in key_metrics.items():
        print(f"  - {metric}: {value:.2f}")

    recommendations = summary.get('recommendations', [])
    if recommendations:
        print("\nğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:")
        for rec in recommendations:
            print(f"  - {rec}")

    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    asyncio.run(main())
